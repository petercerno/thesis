\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

Functional generative description (FGD) is a linguistic framework developed at Charles University in Prague since the 1960s by a team led by Petr Sgall \cite{SgNeGoHa69}. Based on the dependency grammar formalism, it is a stratificational grammar formalism that treats the sentence as a system of interlinked layers: phonological, morphematical, morphonological, analytical (surface syntax) and tectogrammatical (deep syntax).

Analysis by reduction \cite{LoPlKu05} is a technique used in linguistics to analyze sentences of natural languages by using a stepwise simplification of a sentence in such a way that the syntactical correctness or incorrectness of the sentence is preserved. After a finite number of steps either a correct simple sentence is obtained, or an error is detected. In this way it is also possible to determine dependencies between various parts of the given sentence, and to disambiguate between certain morphological ambiguities contained in the sentence. Restarting automaton \cite{JMPV95,O06} was invented to model the analysis by reduction. In fact, many aspects of the work on restarting automata are motivated by the basic tasks of computational linguistics. Several programs are being used in Czech and German (corpus) linguistics that are based on the idea of restarting automata. 

In this thesis we study locally restricted models of restarting automata with the emphasis on the effective learnability of these models. Several restricted models have been introduced and studied intensively, starting with the most restricted model, the so-called clearing restarting automaton \cite{CM10}, which, based on a limited context, can only delete a substring of the current content of its tape. The results obtained so far suggest that these models might be suitable for computational linguistics, since they are quite expressive and, under certain conditions, are identifiable in the limit from an informant (positive and negative samples) \cite{C12}. Moreover, the instructions used in these models are human readable.

The thesis is structured as follows. In Chapter \ref{chapter:background} we provide a short survey of the theory of automata and formal languages. Chapter \ref{chapter:advanced} extends the basics of formal language theory by several selected topics that are important in the context of our own models, such as restarting automata, string-rewriting systems, context rewriting systems, and others. The core of the thesis is split into two Chapters \ref{chapter:crs_nonaux} and \ref{chapter:crs_aux}. Chapter \ref{chapter:crs_nonaux} is devoted to context rewriting systems without auxiliary symbols. As we will see, not using auxiliary symbols often leads to language classes that are not comparable to Chomsky hierarchy (nor other standard classes), but are learnable under a suitable learning paradigm. Chapter \ref{chapter:crs_aux} is focused on context rewriting systems with auxiliary symbols that are hard to learn, but give rise to language classes comparable to other well known language classes.