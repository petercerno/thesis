\chapter{Context Rewriting Systems Without Auxiliary Symbols}\label{chapter:crs_nonaux}

In this chapter we study $\CRS$ without auxiliary symbols. We start with clearing restarting automata in Section \ref{section:clra}, which play a role of the most restricted model. Although clearing restarting automata are very restricted, they can recognize all regular languages, some context-free languages and even some non-context-free languages. However, there are some context-free languages that are outside the class of languages accepted by clearing restarting automata. We also discuss some extensions of clearing restarting automata, such as subword-clearing restarting automata. In Section \ref{section:inference} we study grammatical inference of $\CRS$ without auxiliary symbols from informant. We show that, under certain conditions, it is possible to identify in polynomial time (but not polynomial data) any target $\lambda$-confluent context rewriting system with minimal width of instructions from informant.

\section{Clearing Restarting Automata}\label{section:clra}

The main source for this Section is \cite{C10Diploma, CM10}.

As defined in Definition \ref{definition:derived-classes} / \ref{definition:clra}, a \emph{clearing restarting automaton} \cite{CM10} (\emph{$\clRA$} for short) is a $\CRS$ $M = (\Sigma, \Phi)$, where for each instruction $\phi = (x, z \to t, y) \in \Phi$: $z \in \Sigma^+$ and $t = \lambda$. Clearing restarting automata use only the terminal symbols, which are present in an input word. They use a set of rules for iterated local simplifications until the input word is reduced into the empty word -- in which case the word is accepted -- or a nonempty word that cannot be simplified anymore is obtained -- in which case the word is rejected.

While still being nondeterministic, clearing restarting automata can recognize some non-context-free languages (even without the use of auxiliary symbols).

Clearing restarting automata do not use states. This is similar to stateless restarting automata introduced by Kutrib et al. in \cite{KuMeOt08}. However, a stateless restarting automaton still has the ability to rewrite a subword on its tape by a shorter subword or in its weakest version to delete a scattered subword of its tape. Moreover, a stateless restarting automaton scans its tape from the left to the right and it can check that all the subwords seen in its scanning window are from a finite set of words. Our models of restarting automata can delete only whole subwords.

It is easy to see that an instruction $(x,z,y)$ of a $\clRA$ corresponds to the rewriting meta-instruction $(x',z \to \lambda,y')$ of an $\RR$-automaton, where either $x'=\{x''\}$ in the case when $x=\cent \cdot x''$, or $x' = \Sigma^*\cdot \{x\}$ in the case when $x$ does not start with $\cent$, and either $y'=\{y''\}$ in the case when $y=y'' \cdot \$ $, or $y' = \{y\} \cdot \Sigma^* $ in the case when $y$ does not end with $\$ $.

\begin{theorem}\label{theorem:clRAsubseteqRR}
$\calL{\clRA} \subseteq \calL{\RR}.$
\end{theorem}

As we will see later, the above inclusion is proper. In the following sections we compare the class of languages recognized by clearing restarting automata to the Chomsky hierarchy. We show that the class of languages recognized by clearing restarting automata, while being a proper subset of the class of context-sensitive languages is incomparable to $\CFL$ with respect to inclusion. We study what is the minimal size of alphabet and context used by clearing restarting automata in order to be able to recognize a non-context-free language.

First, in Section \ref{se:clRAandReg} we show that all regular languages can be recognized by clearing restarting automata with restricted left (right, respectively) contexts. In case of a one-letter alphabet, clearing restarting automata recognize exactly all regular languages containing the empty word. In Section \ref{se:nonClosureclRA} we present some (non-)closure properties of $\calL{\clRA}$. We prove that $\calL{\clRA}$ is not closed under several operations (concatenation, intersection, intersection with a regular language, difference) and that there exist context-free languages which cannot be recognized by any $\clRA$.

As we have seen in Theorem \ref{theorem:context_extension}, by increasing the length of contexts used in instructions, $\clRA$ could only increase their power. In Section \ref{se:k-hierarchy_k-cl-RA} we show that the language classes recognized by $\kclRA[k]$ create an infinite proper hierarchy with respect to $k$.

In the following Sections \ref{4clRA-non-CFL}, \ref{1clRA}, \ref{2clRA-non-CFL}, and \ref{3clRA-non-CFL} we study under which restrictions (on the length of contexts and the size of alphabet) $\kclRA[k]$ can recognize non-context-free languages. In these sections, we prefer the generative point of view for clearing restarting automata as was described in Remark \ref{remark:approach}.

In Section \ref{4clRA-non-CFL} we prove that $\kclRA[4]$ are able to recognize a non-context-free language over a two-letter alphabet. We construct a $\kclRA[4]$ $M = (\Sigma, \Phi)$ such that $L(M) \cap \{(ab)^n \mid n > 0\} = \{(ab)^{2^l} \mid l = 0, 1, 2, \ldots\}$. Since context-free languages are closed under intersection with regular languages, it follows that $L(M)$ is a non-context-free language. Rather than describing the language $L(M)$ directly, we use a special inverse homomorphism of this language called \emph{circle-square representation} of $L(M)$. We use the term circle-square representation since we map the language $L(M)$ into a language on alphabet $\{ \Circle, \Square \}$, i.e. the words of this language are composed of circles and squares.

Similar circle-square representations are used also in other subsections of this section. The main motivation behind their use is that they often give us some insight into the structure of the language recognized by the clearing restarting automaton. The language itself is often very difficult to describe directly, but usually it contains some regularities, which are easily captured by using the proper circle-square representation.

In Section \ref{1clRA} we prove that $\kclRA[1]$ can recognize only context-free languages, i.e. $\calL{\kclRA[1]} \subset \CFL$.

In Section \ref{2clRA-non-CFL} we prove that there exists a $\kclRA[2]$ recognizing a non-context-free language over a six-letter alphabet. In the first part we describe a general idea of \emph{sending a signal} through an \emph{ether} and the subsequent \emph{recovery} of the ether by using only instructions of a $\kclRA[2]$. You can imagine the ether as a word with some special properties. The signal is usually a single letter, which spreads through this ether. The propagation of the signal disturbs the ether. Therefore, we have special instructions that recover the disturbed ether. In the second part we look at the process of sending a signal through the ether by using a special circle-square representation. This representation substantially simplifies the original language generated by the process of sending a signal and enables us to prove that this language is a non-context-free language. In the last part we show that it is possible to construct a $\kclRA[2]$ on a six-letter alphabet which simulates the aforementioned process of sending a signal.

In Section \ref{3clRA-non-CFL} we construct a $\kclRA[3]$ $M = (\Sigma, \Phi)$ on a two-letter alphabet $\Sigma = \{a, b\}$ such that $L(M)$ is a non-context-free language. The idea of this automaton is based on the idea of sending a signal through the ether. As you can see this result improves the result from Section \ref{4clRA-non-CFL}. In spite of this fact, we have decided to include Section \ref{4clRA-non-CFL}, since we present there some basic ideas, which are then reused in other subsections in a more complex form.

In Section \ref{clra_membership} we prove that the membership problem for $\clRA$ is $\NP$-complete. Finally, in Section \ref{clra_extensions} we discuss some extensions of $\clRA$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{$\clRA$ and Regular Languages}
\label{se:clRAandReg}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Here we show that clearing restarting automata using only instructions with left contexts starting with the left sentinel $\cent$ recognize exactly the class of regular languages. Further, if we restrict the alphabet used by a $\clRA$ to a single letter, the respective automata recognize regular languages over single-letter alphabet only.

\begin{theorem}\label{theorem:regular_to_clra}
For every $n$-state deterministic finite automaton $A$ there exists an equivalent  clearing restarting automaton $M$ such that $|M| = n+1$. 
\end{theorem}

\begin{proof}
Let $A = (Q, \Sigma, \delta, q_0, F)$ be a deterministic finite automaton, where $Q$ is a set of $n$ states, and let $\delta^*$ be the extension of $\delta$ as defined in Section \ref{subsection:finite-automata}. For every word $w = w_1 \ldots w_n \in \Sigma^n$ there exist words $x_w, z_w, y_w \in \Sigma^*$ such that $w = x_w z_w y_w$, $0 < |z_w| \le n$, and $\delta^*(q_0, x_w) = \delta^*(q_0, x_w z_w)$. This follows from the so-called \emph{pigeon hole principle}: There are only $n$ states and the following sequence: $\delta^*(q_0, \lambda)$, $\delta^*(q_0, w_1)$, $\delta^*(q_0, w_1 w_2)$, $\ldots$, $\delta^*(q_0, w_1 w_2 \ldots w_n)$ contains exactly $n+1$ elements. Therefore, there exist $0 \le i < j \le n$ such that: $\delta^*(q_0, w_1 \ldots w_i) = \delta^*(q_0, w_1 \ldots w_j)$. If we define $x_w = w_1 \ldots w_i$, $z_w = w_{i+1} \ldots w_j$ and $y_w = w_{j+1} \ldots w_n$, then $w = x_w z_w y_w$, $0 < |z_w| \le n$, and $\delta^*(q_0, x_w) = \delta^*(q_0, x_w z_w)$. Now consider a $\clRA$ $M = (\Sigma, \Phi)$, where $\Phi = \{ (\cent, w, \$) \mid w \in L(A)$ and $|w| < n\} \cup \{(\cent x_w, z_w, y_w) \mid w \in \Sigma^n\}$. Apparently, $|M| = n + 1$. Consider any word $w \in \Sigma^*$. If $0 < |w| < n$, then $w \in L(M) \Leftrightarrow w \in L(A)$. Suppose that $|w| \ge n$, i.\,e.\ $w = x_u z_u y_u v$, where $u \in \Sigma^n$ and $v \in \Sigma^*$. Apparently, $x_u z_u y_u v \in L(A) \Leftrightarrow x_u y_u v \in L(A)$. The application of the instruction $(\cent x_u, z_u, y_u) \in \Phi$ on the word $w$ does not change the acceptance of this word by the automaton $A$. It only shortens the length of this word, so after finitely many steps we get a nonempty word that is shorter than $n$ letters.
\end{proof}

On the other hand, the number of instructions of a clearing restarting automaton is not, in general, polynomially bounded with respect to the number of states of an equivalent deterministic finite automaton. We illustrate this in the following Example \ref{example:regular}.

\begin{example}\label{example:regular}
For every positive integer $k > 1$ consider the finite language $L_k = \{ w \in \{a, b\}^* \mid |w|_a = |w|_b = k \}$. It is easy to find a deterministic finite automaton $A_k$ with polynomially many states with respect to $k$ that recognizes the language $L_k$. The automaton $A_k$ only needs to store in its state how many $a$s and $b$s it has read so far, i.\,e.\ it needs only $(k+1)^2$ states. On the other hand, if a $\clRA$ $M = (\Sigma, \Phi)$ recognizes the language $L_k$ then for every word $w \in L_k: (\cent, \underline{w}, \$) \in \Phi$. Proof (by contradiction): Suppose that there exists a word $w \in L_k: (\cent, \underline{w}, \$) \notin \Phi$. No instruction of $M$ can rewrite the whole word $w$ to the empty word in one single step. Therefore, any accepting computation starting from the input word $w$ consists of at least two steps, e.g. $w \vdash_M w' \vdash_M^* \lambda$.  However, the word $w'$ is not an element of $L_k$, because if we clear any proper subword of the word $w$, then we obtain a word that does not have $k$ occurrences of $a$ and $b$. This is a contradiction, because $w' \vdash_M^* \lambda \Rightarrow w' \in L(M)$. We have shown that $|\Phi| \ge |L_k| = \binom{2k}{k}$, which is exponential with respect to $k$.
\end{example}

Nevertheless, there exist regular languages that have a compact representation in the class of clearing restarting automata, but that cannot be compactly represented by using deterministic finite automata. Consider, for instance, the sequence of finite languages $L(M_0)$, $L(M_1)$, $L(M_2)$, $\ldots$ from Example \ref{example:signals}. This sequence of languages has a compact representation in the class of clearing restarting automata because the size of every automaton $M_i$ is polynomial with respect to $i$. On the other hand, for every $i$ the length of the longest word $w_i \in L(M_i)$ is exponential with respect to $i$. Therefore, the smallest deterministic finite automaton $A_i$ recognizing the language $L(M_i)$ must have at least $|w_i|$ states, i.\,e.\ exponentially many states. Otherwise, we could apply the pigeon hole principle on the word $w_i$ which would imply that the language $L(A_i)$ is an infinite language.

Next, we show that the converse also holds if each instruction of the given $\clRA$ is either prefix-rewriting or suffix-rewriting. Rewriting rules, which can rewrite only prefix (suffix, respectively) of tape content, are called prefix-rewriting (suffix-rewriting, respectively). A rule $(x, z, y)$ of a $\clRA$ such that $x$ has prefix $\cent$ can rewrite only the prefix of a tape content, so it is prefix-rewriting. Similarly, each rule $(x, z, y)$ of a $\clRA$ such that $y$ has suffix $\$$ is suffix-rewriting. String rewriting systems having prefix- and suffix-rewriting rules only are called finite combined prefix- and suffix-rewriting systems \cite{Hofbauer2004301}.

\begin{theorem}\label{theorem:clra_to_regular}
Let $M=(\Sigma,\Phi)$ be a $\clRA$ such that for each $(x, z, y) \in \Phi$: $\cent$ is a prefix of $x$ or $\$$ is a suffix of $y$. Then $L(M)$ is a regular language.
\end{theorem}

\begin{proof}
The rewriting relation of $M^D$ satisfying the conditions of the theorem is a finite combined prefix- and suffix rewriting system. Thus, $L(M)=\{ w \in \Sigma^* \mid \lambda \dashv_M^* w \}$ is generated from the regular language $\{\lambda\}$ using a finite combined prefix- and suffix-rewriting system. Hofbauer and Waldman in \cite{Hofbauer2004301} have shown that finite combined prefix- and suffix-rewriting systems preserve regularity. From this it follows immediately that $L(M)$ is a regular language.
\end{proof}

As we have already seen in Example \ref{example:a^n_b^n}, if we allow instructions with left context not starting with $\cent$ and right context not ending with $\$$, then clearing restarting automata can recognize also languages which are not regular.

Also $\clRA$ with a single-letter alphabet without any restrictions on contexts cannot recognize more than regular languages over a single-letter alphabet.

\begin{lemma}
For each $\clRA$ $M = (\Sigma, \Phi)$, where $\Sigma = \{a\}$, $L(M)$ is a regular language.
\end{lemma}

\begin{proof}
If we replace every instruction $\phi = (a^x, a^z, a^y)$ in $\Phi$ by the instruction $\phi' = (\cent \cdot a^x, a^z, a^y)$ we get an equivalent $\clRA$ $M' = (\Sigma, \Phi')$ such that $L(M')=L(M)$ and $u \vdash_M v$ if and only if $u \vdash_{M'} v$. Since for each instruction $\phi' = (x', z', y')$ in $\Phi'$ the left context $x'$ starts with $\cent$, $L(M) = L(M')$ is a regular language according to Theorem \ref{theorem:clra_to_regular}.
\end{proof}

On the other hand by Theorem \ref{theorem:regular_to_clra}, for each regular language $L$ there exists a $\clRA$ $M$ such that $L(M) = L \cup \{\lambda\}$. Since over a single-letter alphabet the class of regular languages equals to the class of context-free languages, we get the following corollary:

\begin{corollary}
If we restrict ourselves to a single-letter alphabet, clearing restarting automata recognize exactly all context-free languages containing the empty word.
\end{corollary}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{(Non-)closure Properties of ${\mathcal L}(\clRA)$}
\label{se:nonClosureclRA}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we present several results showing that the class of languages recognized by clearing restarting automata does not contain all context-free languages and is not closed under several operations.

\begin{theorem}\label{theorem:L_1}
The language $L_1 = \{a^ncb^n \mid n \ge 0\} \cup \{\lambda\}$ is not recognized by any clearing restarting automaton.
\end{theorem}

\begin{proof}
For a contradiction, let us suppose that there exists a $\clRA$ $M = (\Sigma,\Phi)$ such that $L(M) = L_1$. Let $m = |M|$ be the width of $M$. Obviously, $a^m c b^m \in L$ implies $a^m c b^m \vdash_M^* \lambda$ and the word $a^m c b^m$ cannot be reduced to $\lambda$ in a single step. On the other hand, if we erase any single nonempty continuous proper subword from the word $a^m c b^m$, then  we get a word that does not belong to $L_1$ -- a contradiction to $L(M) = L_1$.
\end{proof}

The language $L_1$ can be recognized by a simple $\RR$-automaton. Consequently, using Theorem \ref{theorem:clRAsubseteqRR} and the fact that $L_1$ is a context-free language we get the following:

\begin{corollary}\label{corollary:clRasubsetRRWW}\hspace{1 cm} \
\begin{enumerate}
    \item[a)]
        $\calL{\clRA} \subset \calL{\RR}.$
    \item[b)] \label{co:clRAnotallCFL}
        $\CFL - \calL{\clRA} \not= \emptyset$.
\end{enumerate}
\end{corollary}

Let $L_2 = \{a^nb^n \mid n\ge 0\}$ and $L_3 = \{a^nb^{2n} \mid n \ge 0\}$ be two sample languages. It is easy to see, that both $L_2$ and $L_3$ are recognized by some $\kclRA[1]$.

\begin{theorem}\label{theorem:L_2_L_3}
The languages $L_2 \cup L_3$ and $L_2 \cdot L_3$ are not recognized by any $\clRA$.
\end{theorem}

\begin{proof}
For a contradiction, let us suppose that there exists a $\clRA$ $M = (\Sigma,\Phi)$ such that $L(M) = L_2 \cup L_3$ ($L(M) = L_2 \cdot L_3$, respectively). Let $m = |M|$. Obviously, $a^{4m} b^{4m} \in L(M)$ and $a^{4m} b^{4m} \vdash_M^* \lambda$. Let $a^{4m} b^{4m} \vdash_M^{(\phi)} a^{4m-s} b^{4m-t}$ be the first step of an accepting computation of $M$ on $a^{4m} b^{4m}$, where $s, t \ge 0, s+t > 0$ and $\phi = (x, z, y) \in \Phi$. Because $|\phi| = |xzy| \le m$, $|x|, |y|, |z| \ge 1$ and $a^{4m-s} b^{4m-t} \in L(M)$, it follows that $s = t$, $0 < 2s < m$ and $z = a^sb^s$. Then $a^{4m+s} b^{8m+s} \vdash_M^{(\phi)} = a^{4m} b^{8m} \in L(M)$ and we obtain a contradiction to the error preserving property since $a^{4m+s} b^{8m+s}$ is not in $L_2 \cup L_3$ (not in $L_2 \cdot L_3$, respectively).
\end{proof}

\begin{corollary}
$\calL{\clRA}$ is neither closed under union nor concatenation.
\end{corollary}

\begin{corollary}
$\calL{\clRA}$ is not closed under homomorphism.
\end{corollary}

\begin{proof}
Consider $L = \{ a^n b^n c^m d^{2m} \mid n, m \ge 0 \}$ recognized by $\kclRA[1]$ and the homomorphism $h: a \mapsto a, b \mapsto b, c \mapsto a, d \mapsto b$. Then $h(L) = L_2 \cdot L_3 \not\in \calL{\clRA}$.
\end{proof}

It is easy to see that each of the following languages:
    \begin{eqnarray*}
        L_4 & = & \{a^ncb^n \mid n \ge 0\} \cup \{a^nb^n \mid n \ge 0\}\quad \mbox{ and} \\
        L_5 & = & \{a^ncb^m \mid n, m \ge 0\} \cup \{\lambda\}
    \end{eqnarray*}
can be recognized by a $\kclRA[1]$. Using these languages we can show several additional non-closure properties of $\calL{\clRA}$.

\begin{corollary}
$\calL{\clRA}$ is not closed under
\begin{itemize}
    \item[a)]
        intersection,
    \item[b)]
        intersection with a regular language,
    \item[c)]
        set difference.
\end{itemize}
\end{corollary}

\begin{proof}
Part a) follows from the equality $L_1 = L_4 \cap L_5$ and Theorem \ref{theorem:L_1}. For proving b) just notice that $L_5$ is a regular language. Proof of c) is implied by the equality $L_1 = (L_4 - L_2) \cup \{ \lambda \}$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hierarchy With Respect to the Length of Contexts}
\label{se:k-hierarchy_k-cl-RA}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section we show that by increasing the length of contexts in instructions of clearing restarting automata we strictly increase their power. Hence, we obtain the following infinite hierarchy of language classes.

\begin{theorem}
$\calL{\kclRA} \subset \calL{\kclRA[(k+1)]}$, for all $k\ge 1$.
\end{theorem}

\begin{proof}
First, we show that by increasing the length of contexts for a $\clRA$ we do not decrease its power. If $M = (\Sigma, \Phi)$ is a $\kclRA$, then by Theorem \ref{theorem:context_extension} there exists a \kCRS[(k+1)] $M' = (\Sigma, \Phi')$ such that for each $w, w' \in \Sigma^*:$
$$w \vdash_M w' \quad \Leftrightarrow \quad w \vdash_{M'} w'$$
and both $\Phi$ and $\Phi'$ have the same set of rules. Thus $M'$ is a $\kclRA[(k+1)]$ and $L(M) = L(M')$.

Next we show that by increasing the length of contexts of $\clRA$ we do increase their power. For each $k\ge 1$ and the language $L_1^{(k)} = \{(c^kac^k)^n(c^kbc^k)^n \mid n \ge 0\}$ the following holds:
$$L_1^{(k)} \in \calL{\kclRA[(k+1)]} - \calL{\kclRA}\enspace.$$

It is easy to see that $L_1^{(k)}$ is recognized by the following $\kclRA[(k+1)]$ $M_1^{(k+1)} = (\{a,b,c\},\Phi_1^{(k+1)})$ with instructions:
$$
\begin{array}{l}
(1) \quad (ac^k, \underline{c^kac^kc^kbc^k}, c^kb),\\
(2) \quad (\cent, \underline{c^kac^kc^kbc^k}, \$).
\end{array}
$$
Assume to the contrary that there is a $\kclRA[k]$ $M=(\{a,b,c\},\Phi)$ recognizing the language $L_1^{(k)}$. Let $m = |M|$. The word $w=(c^kac^k)^m(c^kbc^k)^m$ is accepted by $M$. Let us inspect the first reduction of an accepting computation for $w$: $w \vdash_M^{(\phi)} w'$. Then in the instruction $\phi=(x,z,y)$ used in this reduction the deleted part $z$ must be of the form $c^rac^k(c^kac^k)^n(c^kbc^k)^nc^kbc^s$ for some $n, 0 \le n < \frac{m}{6}$, $r,s \ge 0$, $r+s=2k$. The left context $x$ must contain at least one symbol $a$, otherwise the instruction $\phi$ could be applied to the word $w''=(c^kac^k)^m(c^kbc^k)(c^kac^k)^{n+1}(c^kbc^k)^{n+1}(c^kbc^k)^{m-1} \not\in L_1^{(k)}$ and we would obtain $w'' \vdash_M^{(\phi)} w \in L_1^{(k)}$, which contradicts the error preserving property (Theorem \ref{lemma:error-preserving}). Similarly, the right context $y$ must contain at least one symbol $b$. Then obviously $|xzy| \ge 2k+2 +|z|$ and $|x|=|y| \ge k+1$, which is a contradiction.
\end{proof}

\subsection{$\kclRA[4]$ Recognizing a non-Context-Free Language}\label{4clRA-non-CFL}

We have seen that $\clRA$ can recognize some context-free languages. In the following we show that they can even recognize some non-context-free languages. However, since $\calL{\clRA} \subset \calL{\RR}$ and \RR-automata can be simulated by linear bounded automata (\cite{JMPV99}), we have the following:

\begin{corollary}
$\calL{\clRA} \subset \CSL$, where \CSL\ denotes the class of context-sensitive languages.
\end{corollary}

In order to construct a $\clRA$ recognizing a non-context-free language we first describe a scheme for learning instructions of $\clRA$ from a set of reductions. Subsequently we apply this scheme for inferring the desired $\kclRA[4]$.

Knowing some reductions that can be made by an unknown clearing restarting automaton $M$, we can often infer its instructions. Let $w_1 \vdash_M w'_1, \dots, w_n \vdash_M w'_n$, where $w_i,w'_i \in \Sigma^*$ for $i=1, \dots, n$, $n>0$, be a list of known reductions. A meta-algorithm for machine learning of unknown clearing restarting automaton can be outlined as follows:

\begin{algorithm}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\caption{Learning a clearing restarting automaton from a set of sample reductions.}
\label{algorithm:clra-learning}
\DontPrintSemicolon
\LinesNumbered
\Input{Set of reductions $w_i \vdash_M w'_i$, $i = 1, \ldots, n$.}
\Output{$\clRA$ $M$.}
$k := 1$\;
For each reduction $w_i \vdash_M w'_i$ nondeterministically choose a factorization of $w_i$ such that $w_i = \alpha_i \beta_i \gamma_i$ and $w'_i = \alpha_i \gamma_i$.\;\label{clra-step2}
Construct the clearing restarting automaton $M = (\Sigma, \Phi)$, where $\Phi = \bigcup_{i=1}^n \{(\Suff_k(\cent\alpha_i),\beta_i,\Pref_k(\gamma_i\$))\}$.\;
Test the automaton $M$ using any available information, e.g. some negative samples of words not belonging to $L(M)$.\;
If the automaton passed all the tests, \Return{$M$}. \;
Otherwise try another factorization of the known reductions and continue by Step \ref{clra-step2}. If all possible factorizations have been tried, then increase $k$ and continue by Step \ref{clra-step2}.\;
\end{algorithm}

Although Step \ref{clra-step2} is nondeterministic, for many sample reductions the factorization in this step is unambiguous. E.g. for the set of sample reductions $\{aabb \vdash_M ab, ab \vdash_M \lambda\}$ we obtain for $k=1$ only one set of instructions $I=\{(a,ab,b),(\cent,ab,\$)\}$, which is exactly the set of instructions of the automaton $M$ from Example \ref{example:a^n_b^n} recognizing the language $L = \{a^nb^n \mid n\ge 0\}$.

Even if the algorithm is very simple, it can be used to infer non-trivial clearing restarting automata.

In \cite{JMPV97} there was presented a very restricted restarting automaton (deterministic restarting automaton which can delete only) that recognizes a non-context-free language. It is possible to construct a $\clRA$ recognizing a non-context-free language using Algorithm \ref{algorithm:clra-learning} based on reductions collected from a sample computation of this restricted restarting automaton.

\begin{theorem}\label{theorem:clRAnonCFL}
There exists a $\clRA$ $M$ recognizing a non-context-free language.
\end{theorem}

Idea of the proof: We will construct a $\clRA$ $M = (\Sigma, \Phi)$, where $\Sigma = \{a, b\}$, such that $L(M) \cap \{(ab)^n \mid n>0\} = \{(ab)^{2^l} \mid l \ge 0\}$. Since context-free languages are closed under intersection with regular languages, it follows that $L(M)$ is a non-context-free language.

The respective automaton from \cite{JMPV97} accepts the word $(ab)^8$ by the following sequence of reductions:
\begin{eqnarray*}
& & ababababababab\underline{a}b  \vdash_M
ababababab\underline{a}babb  \vdash_M
  ababab\underline{a}babbabb  \vdash_M\\
& & ab\underline{a}babbabbabb  \vdash_M
  abbabbabba\underline{b}b  \vdash_M
  abbabba\underline{b}bab  \vdash_M\\
& & abba\underline{b}babab  \vdash_M
  a\underline{b}bababab  \vdash_M
  ababab\underline{a}b  \vdash_M\\
& & ab\underline{a}babb  \vdash_M
  abba\underline{b}b  \vdash_M
  a\underline{b}bab  \vdash_M\\
& & ab\underline{a}b  \vdash_M
  a\underline{b}b  \vdash_M
  \underline{ab}  \vdash_M
  \lambda  \mbox{ accept}.
\end{eqnarray*}

From this sample computation, we can collect 15 reductions and use them as an input to Algorithm \ref{algorithm:clra-learning}. All these reductions have unambiguous factorizations (the deleted symbols are underlined). Hence, the only variable we have to choose is $k$ -- the length of the context of the instructions. In the following, we use the abbreviation for sets of instructions introduced in Remark \ref{remark:setinstructions}.

\begin{enumerate}
\item For $k = 1$ we get the following 3 instructions:\\
$(b, \underline{a}, b),\quad
 (a, \underline{b}, b),\quad
 (\cent, \underline{ab}, \$)$.\\
Then, however,  the automaton would accept the word $ababab$, which does not belong to $L$: $abab\underline{a}b \vdash ab\underline{a}bb \vdash a\underline{b}bb \vdash a\underline{b}b \vdash \underline{ab} \vdash \lambda$.

\item For $k = 2$ we get the following set of instructions:\\
$(ab, \underline{a}, \{b\$, ba\}),\quad
 (\{\cent a, ba\}, \underline{b}, \{b\$, ba\}),\quad
 (\cent, \underline{ab}, \$)$.\\
Then, however, the automaton would accept the word $ababab$ that does not belong to $L$: $abab\underline{a}b \vdash aba\underline{b}b \vdash ab\underline{a}b \vdash a\underline{b}b \vdash \underline{ab} \vdash \lambda$.

\item For $k = 3$ we get the following set of instructions:\\
$(\{\cent ab, bab\}, \underline{a}, \{b\$, bab\}),\quad
 (\{\cent a, bba\}, \underline{b}, \{b\$, bab\}),\quad
 (\cent, \underline{ab}, \$)$.\\
But then again the automaton would accept the word $ababab$, which does not belong to $L$: $ab\underline{a}bab \vdash a\underline{b}bab \vdash ab\underline{a}b \vdash a\underline{b}b \vdash \underline{ab} \vdash \lambda$.

\item For $k = 4$ we get the following set of instructions:\\
$(\{\cent ab, abab\},\underline{a},\{b\$, babb\}), \quad
(\{\cent a, abba\},\underline{b},\{b\$,bab\$,baba\}), \quad
   (\cent,\underline{ab},\$).$\\
Let us show that this automaton has the required properties.
\end{enumerate}

Let us denote the instructions $\Phi$ of our 4-$\clRA$ $M = (\Sigma, \Phi)$ as:
$$
\begin{array}{ll}
a_1 = (\cent ab, \underline{a}, b\$),\hspace{5em}    & b_3 = (\cent a, \underline{b}, baba),\\
a_2 = (\cent ab, \underline{a}, babb),  & b_4 = (abba, \underline{b}, b\$),\\
a_3 = (abab, \underline{a}, b\$),        & b_5 = (abba, \underline{b}, bab\$),\\
a_4 = (abab, \underline{a}, babb),       & b_6 = (abba, \underline{b}, baba),\\
b_1 = (\cent a, \underline{b}, b\$),     & c_0 = (\cent, \underline{ab}, \$).\\
b_2 = (\cent a, \underline{b}, bab\$),
\end{array}
$$
In the rest of this section we prove that $L(M)$ is not a context-free language. We use the generative approach described in Remark \ref{remark:approach}. The corresponding set of dual instructions $\Phi^D$ can be outlined as (the little vertical arrows in the left-hand sides of these productions mark the positions where some symbols are inserted):
$$
\begin{array}{ll}
A_1: \cent ab ^\downarrow b\$ \dashv_M \cent ab \underline{a} b\$,\hspace{5em} & B_3: \cent a ^\downarrow baba \dashv_M \cent a \underline{b} baba,\\
A_2: \cent ab ^\downarrow babb \dashv_M \cent ab \underline{a} babb,           & B_4: abba ^\downarrow b\$ \dashv_M abba \underline{b} b\$,\\
A_3: abab ^\downarrow b\$ \dashv_M abab \underline{a} b\$,                     & B_5: abba ^\downarrow bab\$ \dashv_M abba \underline{b} bab\$,\\
A_4: abab ^\downarrow babb \dashv_M abab \underline{a} babb ,                  & B_6: abba ^\downarrow baba \dashv_M abba \underline{b} baba,\\
B_1: \cent a ^\downarrow b\$ \dashv_M \cent a \underline{b} b\$,               & C_0: \cent ^\downarrow \$ \dashv_M \cent \underline{ab} \$.\\
B_2: \cent a ^\downarrow bab\$ \dashv_M \cent a \underline{b} bab\$,
\end{array}
$$
Now observe that each word generated by $M$ is of the form:
$$w = (ab)^{x_0}(abb)^{y_0} \ldots (ab)^{x_n}(abb)^{y_n},$$
where $n \ge 0, x_0 \ge 0, y_0 > 0, \ \ x_1 > 0, y_1 > 0, \ \ \ldots, \ \ x_n > 0, y_n \ge 0$ (for $n = 0$ we consider for $x_0$ and $y_0$ only the inequalities $x_0 \ge 0, y_0 \ge 0$). This is because each production preserves this form. This form allows us to define the so-called \emph{circle-square representation} of $w$ as $\Circle^{x_0} \Square^{y_0} \ldots \Circle^{x_n} \Square^{y_n}$ where each circle $\Circle$ represents $ab$ and each square $\Square$ represents $abb$. If we rewrite the productions of $M$ in this circle-square representation, we get:
$$
\begin{array}{ll}
A_1': \cent \underline{\Square} \$ \dashv_M \cent \Circle \Circle \$,\hspace{5em}    & B_3': \cent \underline{\Circle} \Circle ? \dashv_M \cent \Square \Circle ?,\\
A_2': \cent \underline{\Square} \Square \dashv_M \cent \Circle \Circle \Square,      & B_4': \Square \underline{\Circle} \$ \dashv_M \Square \Square \$, \\
A_3': \Circle \underline{\Square} \$ \dashv_M \Circle \Circle \Circle \$ ,           & B_5': \Square \underline{\Circle} \Circle \$ \dashv_M \Square \Square \Circle \$, \\
A_4': \Circle \underline{\Square} \Square \dashv_M \Circle \Circle \Circle \Square,  & B_6': \Square \underline{\Circle} \Circle ? \dashv_M \Square \Square \Circle ?,\\
B_1': \cent \underline{\Circle} \$ \dashv_M \cent \Square \$,                        & C_0': \cent ^\downarrow \$ \dashv_M \cent \Circle \$ ,\\
B_2': \cent \underline{\Circle} \Circle \$ \dashv_M \cent \Square \Circle \$,
\end{array}
$$
where the symbol $?$ represents either the symbol $\Circle$, or the symbol $\Square$.

\begin{lemma}
Consider a $\kCRS[1]$ $R = (\{\Circle, \Square\}, \Omega)$ with instructions $\Omega$:
$$\begin{array}{l}
(0) \quad (\cent, \lambda \to \Circle, \$),\\
(1) \quad (\{\cent, \Square\}, \underline{\Circle} \to \Square, \{\Circle, \$\}) \text{, and}\\
(2) \quad (\{\cent, \Circle\}, \underline{\Square} \to \Circle \Circle, \{\Square, \$\}).
\end{array}
$$
Then for every $\phi \in \Phi^D$: $\phi$ can be applied to $w = (ab)^{x_0}(abb)^{y_0} \ldots (ab)^{x_n}(abb)^{y_n}$ if and only if there exists $\omega \in \Omega$ which can be applied in the same way to the word $\Circle^{x_0} \Square^{y_0} \ldots \Circle^{x_n} \Square^{y_n}$. Thus the circle-square representation of the language $L(M)$ is equal to $\{ w \mid \lambda \vdash_R^* w \} = L(R^D)$.
\end{lemma}

\begin{proof}
There is one-to-one correspondence between the productions $A_1$, $A_2$, $A_3$, $A_4$, $B_1$, $B_4$, $C_0$ from $\Phi^D$ and instructions $A_1'$, $A_2'$, $A_3'$, $A_4'$, $B_1'$, $B_4'$, $C_0'$ from $\Omega$. Now observe that the productions $B_2$ and $B_3$ correspond to the instruction $(\cent, \underline{\Circle} \to \Square, \Circle)$ of $R$, and similarly the productions $B_5$ and $B_6$ correspond to the instruction $(\Square, \underline{\Circle} \to \Square, \Circle)$ of $R$.
\end{proof}

The following theorem characterizes the languages recognized by a slightly more general version of the $\kCRS[1]$ from the previous lemma. Moreover, we will use this theorem also in another section in a different situation.

\begin{theorem}\label{theorem:circle-square-theorem}
Consider a $\kCRS[1]$ $R = (\{\Circle, \Square\}, \Omega)$ with instructions:
$$
\begin{array}{l}
(0) \quad (\cent, \lambda \to \Circle^m, \$),\\
(1) \quad (\{\cent, \Square\}, \underline{\Circle} \to \Square^{\alpha}, \{\Circle, \$\})\text,\\
(2) \quad (\{\cent, \Circle\}, \underline{\Square} \to \Circle^{\beta}, \{\Square, \$\}),
\end{array}
$$
where $m, \alpha, \beta \ge 1$ are integer constants. Then every $w \in L(R^D)$ is of the form:\\
$$w = \Circle^{x_0} \Square^{y_0} \Circle^{x_1} \Square^{y_1} \ldots \Circle^{x_k} \Square^{y_k},$$
where:
$$
\begin{array}{c}
k \ge 0, x_0 \ge 0, y_0 > 0, \ \ x_1 > 0, y_1 > 0, \ \ \ldots, \ \ x_k > 0, y_k \ge 0,\\
\alpha^0 \beta^0 x_0 + \alpha^0 \beta^1 y_0 + \alpha^1 \beta^1 x_1 + \alpha^1 \beta^2 y_1 + \ldots +
\alpha^k \beta^k x_k + \alpha^k \beta^{k+1} y_k = (\alpha \beta)^l m
\end{array} \quad (*)
$$
for some $l \ge 1$. We call this sum the \emph{overall sum} of the word $w$. Note that in the case $k = 0$ we consider for $x_0$ and $y_0$ only the inequalities $x_0 \ge 0, y_0 \ge 0$.
\end{theorem}

\begin{proof}
(By induction on the number of used instructions)

The word $\Circle^m$ evidently
satisfies all conditions of the theorem, i.e. $k = 0$, $x_0 = m$, $y_0 = 0$, and
$\alpha^0 \beta^0 x_0 + \alpha^0 \beta^1 y_0 = (\alpha \beta)^0 m$.

Suppose that we have $w$ of the form
$w = \Circle^{x_0} \Square^{y_0} \Circle^{x_1} \Square^{y_1} \ldots \Circle^{x_k} \Square^{y_k}$
satisfying all conditions $(*)$ of the theorem.
Depending on the last used instruction, there are several cases we have to consider:

\begin{enumerate}
\item $(\cent, \underline{\Circle} \to \Square^{\alpha}, \$)$:
can be applied only to the word $w = \Circle$, i.e. $m = 1$. We get $w' = \Square^{\alpha}$ satisfying all conditions $(*)$, i.e. $k = 0$, $x_0 = 0$, $y_0 = \alpha$, and $\alpha^0 \beta^0 x_0 + \alpha^0 \beta^1 y_0 = \alpha \beta = (\alpha \beta) m$.

\item $(\cent, \underline{\Circle} \to \Square^{\alpha}, \Circle)$:
can be applied to $w = \underline{\Circle} \Circle^{x_0 - 1} \Square^{y_0} \Circle^{x_1} \Square^{y_1} \ldots \Circle^{x_k} \Square^{y_k}$, where $x_0 \ge 2$. We get $w' = \Square^{\alpha} \Circle^{x_0 - 1} \Square^{y_0} \Circle^{x_1} \Square^{y_1} \ldots \Circle^{x_k} \Square^{y_k}$, which is equal to $\Circle^{x'_0} \Square^{y'_0} \Circle^{x'_1} \Square^{y'_1} \ldots \Circle^{x'_{k+1}} \Square^{y'_{k+1}}$, where $x'_0 = 0$, $y'_0 = \alpha$, $x'_1 = x_0 - 1$, $y'_1 = y_0$, and $x'_{i+1} = x_i$, $y'_{i+1} = y_i$ for all $i \in \{1, 2, \ldots, k\}$. Thus $\alpha^0 \beta^0 x'_0 + \alpha^0 \beta^1 y'_0 + \alpha^1 \beta^1 x'_1 + \alpha^1 \beta^2 y'_1 + \ldots \alpha^{k+1} \beta^{k+1} x'_{k+1} + \alpha^{k+1} \beta^{k+2} y'_{k+1} = \alpha^0 \beta^0 \times 0 + \alpha^0 \beta^1 \times \alpha - \alpha^1 \beta^1 \times 1 + \alpha^1 \beta^1 (\alpha^0 \beta^0 x_0 + \alpha^0 \beta^1 y_0 + \ldots \alpha^k \beta^k x_k + \alpha^k \beta^{k+1} y_k) = (\alpha \beta) \times (\alpha \beta)^l m = (\alpha \beta)^{l+1} m$.

\item $(\Square, \underline{\Circle} \to \Square^{\alpha}, \$)$:
can be applied to $w = \Circle^{x_0} \Square^{y_0} \ldots \Circle^{x_{k-1}} \Square^{y_{k-1}} \underline{\Circle}$, where $k \ge 1$,  $y_{k-1} \ge 1$, $x_k = 1$, $y_k = 0$. We get $w = \Circle^{x_0} \Square^{y_0} \ldots \Circle^{x_{k-1}} \Square^{y_{k-1}} \Square^{\alpha} = \Circle^{x'_0} \Square^{y'_0} \ldots \Circle^{x'_{k-1}} \Square^{y'_{k-1}}$, where $x'_i = x_i$, $y'_i = y_i$ for all $i \in \{0, 1, \ldots, k-2\}$, and $x'_{k-1} = x_{k-1}$, $y'_{k-1} = y_{k-1} + \alpha$. Thus $\alpha^0 \beta^0 x'_0 + \alpha^0 \beta^1 y'_0 + \alpha^1 \beta^1 x'_1 + \alpha^1 \beta^2 y'_1 + \ldots \alpha^{k-1} \beta^{k-1} x'_{k-1} + \alpha^{k-1} \beta^k y'_{k-1} = \alpha^0 \beta^0 x_0 +  \alpha^0 \beta^1 y_0 + \alpha^1 \beta^1 x_1 + \alpha^1 \beta^2y_1 + \ldots \alpha^{k-1} \beta^{k-1} x_{k-1} + \alpha^{k-1} \beta^k (y_{k-1} + \alpha) = \alpha^0 \beta^0 x_0 + \alpha^0 \beta^1 y_0 + \ldots \alpha^{k-1} \beta^{k-1} x_{k-1} + \alpha^{k-1} \beta^k y_{k-1} + \alpha^k \beta^k x_k + \alpha^k \beta^{k+1} y_k = (\alpha \beta)^l m$.

\item $(\Square, \underline{\Circle} \to \Square^{\alpha}, \Circle)$:
can be applied to a subword $u = \Square \underline{\Circle} \Circle$ of the word $w$. We get a new subword $u' = \Square \Square^{\alpha} \Circle$ and the corresponding $w'$. The overall sum does not change, since each $\Square$ in both subwords contributes to this sum with the weight $\alpha^i \beta^{i+1}$ for some $i \ge 0$ and each $\Circle$ contributes to this sum with the weight $\alpha^{i+1} \beta^{i+1}$. Thus $w'$ satisfies all conditions $(*)$ of the theorem.

\item $(\cent, \underline{\Square} \to \Circle^{\beta}, \$)$:
can be applied only to the word $w = \Square = \Circle^{x_0} \Square^{y_0}$, where $k = 0$, $x_0 = 0$, $y_0 = 1$. We get $w' = \Circle^{\beta} = \Circle^{x_0'} \Square^{y_0'}$, where $x_0' = \beta$, $y_0' = 0$, and the overall sum $\alpha^0 \beta^0 x_0' + \alpha^0 \beta^1 y_0' = \beta = \alpha^0 \beta^0 x_0 + \alpha^0 \beta^1 y_0$ remains unchanged.

\item $(\cent, \underline{\Square} \to \Circle^{\beta}, \Square)$:
can be applied to $w = \underline{\Square} \Square^{y_0 - 1} \Circle^{x_1} \Square^{y_1} \ldots \Circle^{x_k} \Square^{y_k}$, where $x_0 = 0$ and $y_0 \ge 2$. We get $w' = \Circle^{\beta} \Square^{y_0 - 1} \Circle^{x_1} \Square^{y_1} \ldots \Circle^{x_k} \Square^{y_k}$ which is equal to $\Circle^{x'_0} \Square^{y'_0} \Circle^{x'_1} \Square^{y'_1} \ldots \Circle^{x'_k} \Square^{y'_k}$, where $x'_0 = \beta$, $y'_0 = y_0 - 1$, and $x'_i = x_i$, $y'_i = y_i$ for all $i \in \{1, 2, \ldots, k\}$. Thus $\alpha^0 \beta^0 x'_0 + \alpha^0 \beta^1 y'_0 + \alpha^1 \beta^1 x'_1 + \alpha^1 \beta^2 y'_1 + \alpha^k \beta^k x'_k + \alpha^k \beta^{k+1} y'_k = \alpha^0 \beta^0 \times \beta + \alpha^0 \beta^1 (y_0 - 1) + \alpha^1 \beta^1 x_1 + \alpha^1 \beta^2 y_1 + \ldots \alpha^k \beta^k x_k + \alpha^k \beta^{k+1} y_k = (\alpha \beta)^l m$.

\item $(\Circle, \underline{\Square} \to \Circle^{\beta}, \$)$:
can be applied to $w = \Circle^{x_0} \Square^{y_0} \Circle^{x_1} \Square^{y_1} \ldots \Circle^{x_k} \underline{\Square}$, where $x_k \ge 1$, $y_k = 1$. We get $w' = \Circle^{x_0} \Square^{y_0} \Circle^{x_1} \Square^{y_1} \ldots \Circle^{x_k} \Circle^{\beta} = \Circle^{x'_0} \Square^{y'_0} \Circle^{x_1} \Square^{y_1} \ldots \Circle^{x'_k} \Square^{y'_k}$, where $x'_i = x_i$, $y'_i = y_i$ for all $i \in \{0, 1, \ldots, k-1\}$, $x'_k = x_k + \beta$, and $y'_k = 0$. Thus $\alpha^0 \beta^0 x'_0 + \alpha^0 \beta^1 y'_0 + \ldots \alpha^{k-1} \beta^{k-1} x'_{k-1} + \alpha^{k-1} \beta^k y'_{k-1} + \alpha^k \beta^k x'_k + \alpha^k \beta^{k+1} y'_k = \alpha^0 \beta^0 x_0 + \alpha^0 \beta^1 y_0 + \ldots \alpha^{k-1} \beta^{k-1} x_{k-1} + \alpha^{k-1} \beta^k y_{k-1} + \alpha^k \beta^k (x_k + \beta) = (\alpha \beta)^l m$.

\item $(\Circle, \underline{\Square} \to \Circle^{\beta}, \Square)$:
can be applied to a subword $u = \Circle \underline{\Square} \Square$ of the word $w$. We get a new subword $u' = \Circle \Circle^{\beta} \Square$ and the corresponding $w'$. The overall sum does not change, since each $\Circle$ in both subwords contributes to this sum with the weight $\alpha^i \beta^i$ for some $i \ge 0$ and each $\Square$ contributes to this sum with the weight $\alpha^i \beta^{i+1}$. Thus $w'$ satisfies all conditions $(*)$ of the theorem.
\end{enumerate}
\end{proof}

\begin{theorem}\label{theorem:circle-square-cap-theorem}
Let $R$ be a $\kCRS[1]$ from Theorem \ref{theorem:circle-square-theorem}. Then
$$L(R^D) \cap \{\Circle\}^+ = \{\Circle^{x} \mid x = (\alpha \beta)^l m, \ l = 0, 1, 2, \ldots\}.$$
\end{theorem}

\begin{proof}
If $\lambda \vdash_R^* \Circle^{x}$, then by Theorem \ref{theorem:circle-square-theorem} necessarily $x = (\alpha \beta)^l m$ for some $l \ge 0$. On the other hand, by using the instructions of $R$ we can easily generate any such word $\Circle^{x}$, where $x = (\alpha \beta)^l m$. For instance, consider the following derivation:\\
$\Circle^m = \underline{\Circle} \Circle^{m-1} \vdash_R \Square^{\alpha} \underline{\Circle} \Circle^{m-2} \vdash_R
\Square^{\alpha} \Square^{\alpha} \underline{\Circle} \Circle^{m-3} \vdash_R^*
\Square^{\alpha  (m-1)} \underline{\Circle} \vdash_R \Square^{\alpha  m} = \\
\underline{\Square} \Square^{\alpha  m - 1} \vdash_R
\Circle^{\beta} \underline{\Square} \Square^{\alpha  m - 2} \vdash_R
\Circle^{\beta} \Circle^{\beta} \underline{\Square} \Square^{\alpha  m - 3} \vdash_R^*
\Circle^{\beta (\alpha  m - 1)} \underline{\Square} \vdash_R \Circle^{(\alpha \beta)  m}$.\\
Thus $\Circle^m \vdash_R^* \Circle^{(\alpha \beta)  m}$, which implies that $\Circle^m \vdash_R^* \Circle^{(\alpha \beta)^l  m}$ for any $l \ge 0$.
\end{proof}

If we apply Theorem \ref{theorem:circle-square-cap-theorem} to the circle-square representation of $L(M)$, i.e. $m = 1$, $\alpha = 1$, and $\beta = 2$, we get the following result:

\begin{theorem}\label{theorem:4clRA}
$L(M) \cap \{(ab)^n \mid n > 0\} = \{(ab)^{2^l} \mid l = 0, 1, 2, \ldots\}$.
\end{theorem}

As context-free languages are closed under intersection with regular languages, $L(M)$ is not a context-free language.

\subsection{$\kclRA[1]$ Recognize at most Context-Free Languages}\label{1clRA}

Let $M = (\Sigma, \Phi)$ be a $\clRA$, $l \in \{ \cent, \lambda \} \cdot \Sigma^*$, and $r \in \Sigma^* \cdot \{ \lambda, \$ \}$. Let us denote $L_{(l,r)}(M) = \{w \in \Sigma^* \mid w \vdash_M^* \lambda$ in the context $(l,r) \}$ (see Definition \ref{definition:crs}). If $\clRA$ $M$ is known from the context, then we use the abbreviation $L_{(l,r)} = L_{(l,r)}(M)$.

\begin{example}
Suppose that we have $\kclRA[1]$ $M = (\Sigma, \Phi)$, where $\Sigma = \{a, b\}$ and the instructions are $\Phi = \{(\cent, \underline{ab}, \$), (a, \underline{ab}, b)\}$. Of course, we have\\
\indent $L_{({\small \cent},\$)}(M) = \{\lambda\} \cup a \cdot L_{(a,b)}(M) \cdot b$ \ and\\
\indent $L_{(a,b)}(M) = \{\lambda\} \cup a \cdot L_{(a,b)}(M) \cdot b$.\\
We can rewrite these language equations into the context-free grammar $G = (V_N, V_T, S, P)$ with $V_N = \{S, X\}$, $V_T = \Sigma$, and the following set of production rules:\\
\indent $S \to \lambda \mid a X b$,\\
\indent $X \to \lambda \mid a X b$.\\
We can generalize this technique to any $\kclRA[1]$.
\end{example}

\begin{lemma}\label{lemma:1clra}
Suppose that we have $\kclRA[1]$ $M = (\Sigma, \Phi)$.
Then for each $l \in LC_1 = \{\cent\} \cup \Sigma$, and $r \in RC_1 = \Sigma \cup \{\$\}$:
$$L_{(l,r)} = \{\lambda\} \cup \bigcup_{(l, u_1 \ldots u_n, r) \in \Phi, u_1,\ldots,u_n \in \Sigma}
L_{(l, u_1)} \cdot u_1 \cdot L_{(u_1, u_2)} \cdot u_2 \ldots u_{n-1} \cdot
L_{(u_{n-1}, u_n)} \cdot u_n \cdot L_{(u_n, r)}\enspace.$$
\end{lemma}

\begin{proof}
Let $R$ denote the right-hand side of this equation. Suppose $w \in L_{(l,r)}$. If $w = \lambda$, then $w \in R$. If $w \neq \lambda$, then $w \vdash_M^* w_0 \vdash_M \lambda$ in the context $(l,r)$. Suppose $w_0 = u_1 \ldots u_n$. Then, clearly, $(l, u_1 \ldots u_n, r) \in \Phi$ and $w = z_0 u_1 z_1 u_2 z_2 \ldots u_n z_n$ for some $z_0, \ldots, z_n \in \Sigma^*$ such that $z_0 \vdash_M^* \lambda$ in the context $(l, u_1)$, $z_1 \vdash_M^* \lambda$ in the context $(u_1, u_2)$, etc., $z_n \vdash_M^* \lambda$ in the context $(u_n, r)$. This is equivalent to $z_0 \in L_{(l,u_1)}$, $z_1 \in L_{(u_1, u_2)}$, \dots, $z_n \in L_{(u_n, r)}$. Thus $w$ is in  $R$.

Suppose that $w \in R$. If $w = \lambda$, then $w \in L_{(l,r)}$. Otherwise, suppose that $w = z_0 u_1 z_1 u_2 z_2 \ldots u_n z_n$ for some $(l, u_1 \ldots u_n, r) \in \Phi$ and $z_0, \ldots, z_n \in \Sigma^*$ such that $z_0 \in L_{(l,u_1)}$, $z_1 \in L_{(u_1, u_2)}$,\dots, $z_n \in L_{(u_n, r)}$. Obviously, $w = \underline{z_0} u_1 z_1 u_2 z_2 \ldots u_n z_n \vdash_M^* u_1 \underline{z_1} u_2 z_2 \ldots u_n z_n \vdash_M^* u_1 u_2 \underline{z_2} \ldots u_n z_n \vdash_M^* \ldots \vdash_M^* u_1 u_2 \ldots u_n \underline{z_n} \vdash_M^* u_1 u_2 \ldots u_n \vdash_M \lambda$ in the context $(l,r)$ and hence $w \in L_{(l,r)}$.
\end{proof}

Since we can easily rewrite the language equations from Lemma \ref{lemma:1clra}
into the production rules of a context-free grammar with the initial nonterminal corresponding to $L_{({\small \cent},\$)}$,
we get together with Theorem \ref{theorem:L_1} the following corollary:

\begin{corollary}\label{corollary:1clra_cfl}
$\calL{\kclRA[1]} \subset \CFL$\ . Moreover, for every $\kclRA[1]$ $M$ the context-free grammar $G$, s. t. $L(G) = L(M)$, can be effectively constructed in polynomial time w.r.t. the size of $M$.
\end{corollary}

\subsection{$\kclRA[2]$ Recognizing a non-Context-Free Language}\label{2clRA-non-CFL}

In the following, we use the generative approach as described in Remark \ref{remark:approach}. We restrict ourselves to $\kclRA[2]$ only.

\begin{definition}\label{definition:ether}
Let $\Lambda$ be a finite nonempty set of the so-called \emph{basic symbols}. We define a set of the so-called \emph{wave symbols} as $\wave(\Lambda) = \{\stackrel{uv}{\sim} \mid u,v \in \Lambda\}$. Let $\Sigma = \Lambda \cup \wave(\Lambda)$.

We say that a couple $(x, y) \in \Sigma \times \Sigma$ satisfies the \emph{ether property} if and only if the couple $(x, y)$ is one of the following types:
$$
\begin{array}{ll}
(1) \quad (\stackrel{uv}{\sim}, \stackrel{vw}{\sim}) 	& \mbox{for some } u, v, w \in \Lambda,\\
(2) \quad (\stackrel{uv}{\sim}, v)					& \mbox{for some } u, v \in \Lambda,\\
(3) \quad (u, \stackrel{uv}{\sim})					& \mbox{for some } u, v \in \Lambda.
\end{array}
$$
For each wave symbol $x =\ \stackrel{uv}{\sim}$ let us define ${\sf left}(x) = u$, ${\sf right}(x) = v$. For $x \in \Lambda$ let us define ${\sf left}(x) = {\sf right}(x) = x$. Hence, a couple $(x, y) \in \Sigma \times \Sigma$ satisfies the \emph{ether property} if and only if ${\sf right}(x) = {\sf left}(y)$ and at least one of the symbols $x, y$ is a wave symbol.

We say that a word $w = x_1 x_2 \ldots x_n \in \Sigma^*$ satisfies the \emph{ether property} if and only if all couples $(x_1, x_2)$, $(x_2, x_3)$, ..., $(x_{n-1}, x_n)$ satisfy the ether property. We also say that the word $w$ is an \emph{ether} and for clarity we often mark this property by a line above the ether, e.g. $\overline{a \stackrel{ab}{\sim} \; \stackrel{bc}{\sim}}\ \overline{a \stackrel{ab}{\sim} b \stackrel{bc}{\sim} c}\ a$.
\end{definition}

First, we will informally describe the motivation behind Definition \ref{definition:ether}. Suppose that we have two special symbols $X, Y \in \Lambda$ -- the so-called \emph{membranes} -- connected by an ether, i.e. a word satisfying the ether property, for instance $\overline{X \stackrel{Xe}{\sim} \; \stackrel{ee}{\sim} \; \stackrel{eY}{\sim} Y}$. We want to send some signal from $X$ to $Y$ through the ether. For technical reasons, the signal is represented by two special basic symbols $a, \tilde{a} \in \Lambda$. After sending a signal, we want to recover the ether between $X$ and $Y$ in order to be able to send another signal. Our goal is to simulate this process by using a $\kclRA[2]$ $M$. The choice of the term membrane for the symbols $X$ and $Y$ is motivated by the fact that a membrane usually serves as a separator of two spaces. In our case the membranes $X$ and $Y$ are on the border of the space filled by the ether, but in general we can have more membranes such that each two consecutive membranes are connected by different ethers.

We can schematically describe the process of spreading a signal from left to right in the ether as:
$$\sigma \overline{x y z} \dashv_M \sigma x \sigma' \overline{y z}\;,$$
where $\sigma, \sigma' \in \{a, \tilde{a}\}$. We call this step a \emph{jump} of the signal $\sigma$ \emph{from left to right}. The jump can occur only if the couple $(\sigma, x)$ does not satisfy the ether property and both couples $(x, y)$ and $(y, z)$ do satisfy the ether property. We have to choose $\sigma'$ in such a way that neither $(x, \sigma')$, nor $(\sigma', y)$ satisfy the ether property. It is easy to see that we can always choose such $\sigma' \in \{a, \tilde{a}\}$. If ${\sf right}(x) = {\sf left}(y) = a$, then choose $\sigma' = \tilde{a}$. Otherwise, if ${\sf right}(x) = {\sf left}(y) \neq a$, then choose $\sigma' = a$.

Now observe that we can simulate the jump of the signal $\sigma$ from left to right by the dual instruction to the instruction:
$$(\sigma x, \sigma', yz)\;,$$
which is a legal instruction of a $\kclRA[2]$.

\begin{example}\label{example:spread}
In this example we demonstrate the process of sending a signal from $X$ to $Y$:
$$
\begin{array}{ll}
(1) & \quad \overline{X \stackrel{Xe}{\sim} \; \stackrel{ee}{\sim} \; \stackrel{eY}{\sim} Y} \dashv_M\\
(2) & \quad X\ \underline{a}\ \overline{\stackrel{Xe}{\sim} \; \stackrel{ee}{\sim} \; \stackrel{eY}{\sim} Y} \dashv_M\\
(3) & \quad X\ a \stackrel{Xe}{\sim} \underline{a} \; \overline{\stackrel{ee}{\sim} \; \stackrel{eY}{\sim} Y} \dashv_M\\
(4) & \quad X\ a \stackrel{Xe}{\sim} a \stackrel{ee}{\sim} \underline{a}\; \overline{\stackrel{eY}{\sim} Y} \dashv_M\\
(5) & \quad X\ a \stackrel{Xe}{\sim} a \stackrel{ee}{\sim} a \stackrel{eY}{\sim} \underline{a}\ Y\;.
\end{array}
$$
Note that in the first step $(1) \to (2)$ the symbol $X$ initiated sending a signal $a$ to $Y$. On the other hand, in the last step $(4) \to (5)$ the symbol $Y$ received the signal $a$ from $X$. Only in the steps $(2) \to (3)$ and $(3) \to (4)$ we used the jump operation.
Now we have to recover the ether between $X$ and $Y$ to be able to send another signal.
\end{example}

We can schematically describe the process of recovering an ether
from left to right as:
$$\overline{x y} z d \dashv_M \overline{x y \stackrel{uv}{\sim} z} d,$$
where $u = {\sf right}(y)$ and $v = {\sf left}(z)$. We call this step a \emph{recovery} of the ether \emph{from left to right}. The recovery can occur only if the couple $(x, y)$ satisfies the ether property and neither $(y, z)$, nor $(z, d)$ satisfy the ether property.

Once again, observe that we can simulate the recovery of the ether from left to right by the dual instruction to the instruction:
$$(xy, \stackrel{uv}{\sim}, zd),$$
which is a legal instruction of a $\kclRA[2]$.

\begin{example}\label{example:recover}
In this example, we demonstrate the process of recovering the ether from $X$ to $Y$. We continue from the previous Example \ref{example:spread}.
$$
\begin{array}{ll}
(5) & X \; a \stackrel{Xe}{\sim} a \stackrel{ee}{\sim} a \stackrel{eY}{\sim} a \; Y \dashv_M\\
(6) & \overline{X \stackrel{Xa}{\sim} a}\ \stackrel{Xe}{\sim}\ a\ \stackrel{ee}{\sim}\ a\ \stackrel{eY}{\sim}\ a\ Y \dashv_M\\
(7) & \overline{X \stackrel{Xa}{\sim} a \stackrel{aX}{\sim} \, \stackrel{Xe}{\sim}}\ a\ \stackrel{ee}{\sim}\ a\ \stackrel{eY}{\sim}\ a\ Y \dashv_M\\
(8) & \overline{X \stackrel{Xa}{\sim} a \stackrel{aX}{\sim} \, \stackrel{Xe}{\sim} \; \stackrel{ea}{\sim} a}\
\stackrel{ee}{\sim}\ a\ \stackrel{eY}{\sim}\ a\ Y \dashv_M\\
(9) & \overline{X \stackrel{Xa}{\sim} a \stackrel{aX}{\sim} \, \stackrel{Xe}{\sim} \; \stackrel{ea}{\sim} a
\stackrel{ae}{\sim} \ \stackrel{ee}{\sim}}\ a\ \stackrel{eY}{\sim}\ a\ Y \dashv_M\\
(10) & \overline{X \stackrel{Xa}{\sim} a \stackrel{aX}{\sim} \, \stackrel{Xe}{\sim} \; \stackrel{ea}{\sim} a
\stackrel{ae}{\sim} \, \stackrel{ee}{\sim} \, \stackrel{ea}{\sim} a}\ \stackrel{eY}{\sim}\ a\ Y \dashv_M\\
(11) & \overline{X \stackrel{Xa}{\sim} a \stackrel{aX}{\sim} \, \stackrel{Xe}{\sim} \; \stackrel{ea}{\sim} a
\stackrel{ae}{\sim} \, \stackrel{ee}{\sim} \, \stackrel{ea}{\sim} a \stackrel{ae}{\sim} \, \stackrel{eY}{\sim}}\ a\ Y \dashv_M\\
(12) & \overline{X \stackrel{Xa}{\sim} a \stackrel{aX}{\sim} \, \stackrel{Xe}{\sim} \; \stackrel{ea}{\sim} a
\stackrel{ae}{\sim} \, \stackrel{ee}{\sim} \, \stackrel{ea}{\sim} a \stackrel{ae}{\sim} \, \stackrel{eY}{\sim} \, \stackrel{Ya}{\sim} a}\ Y \dashv_M\\
(13) & \overline{X \stackrel{Xa}{\sim} a \stackrel{aX}{\sim} \, \stackrel{Xe}{\sim} \; \stackrel{ea}{\sim} a
\stackrel{ae}{\sim} \, \stackrel{ee}{\sim} \, \stackrel{ea}{\sim} a \stackrel{ae}{\sim} \, \stackrel{eY}{\sim} \, \stackrel{Ya}{\sim} a \stackrel{aY}{\sim} Y}\;.
\end{array}
$$
Note that in the first step $(5) \to (6)$ the symbol $X$ initiated recovering of the ether between $X$ and $Y$. On the other hand, in the last step $(12) \to (13)$ the symbol $Y$ finished the recovery of the ether between $X$ and $Y$. Only in the steps $(6) \to (7)$ to $(11) \to (12)$ we used the recovery operation. Now we can send another signal from $X$ to $Y$.
\end{example}

It is difficult to describe the language $L$ which is generated from the word $X \stackrel{Xe}{\sim} \; \stackrel{ee}{\sim} \; \stackrel{eY}{\sim} Y$ by repeated sending a signal (represented by the symbols $a$ and $\tilde{a}$) from $X$ to $Y$, as in Example \ref{example:spread} and Example \ref{example:recover}. But observe that if the ether between $X$ and $Y$ has $n$ symbols, then after sending a signal through this ether we get an unusable transfer medium between $X$ and $Y$ of length $2n + 1$. After subsequent recovering of this medium we get a new ether between $X$ and $Y$ of length $2(2n+1) + 1$. Thus the transfer medium between $X$ and $Y$ grows exponentially as we send signals from $X$ to $Y$. Also note that $L_{ether} = \{w \in \Sigma^* \mid |w| \ge 2, w \text{ is an ether }\}$ is a regular language. It can be shown that the language $L \cap L_{ether}$ contains only words of length $4^k + 1$, and  for each $k \ge 1$ it containes at least one word. This implies that $L$ is not a context-free language. In order to prove this formally we need to make some observations.

Suppose that we have $w = a_1 a_2 \ldots a_n \in \Sigma^*$. Consider the sequence of couples $(a_1, a_2)$, $(a_2, a_3)$, ..., $(a_{n-1}, a_n)$. Now if we replace each couple in this sequence with the symbol $\Circle$ or $\Square$ depending on whether the
corresponding couple satisfies the ether property or not, we get a so-called \emph{circle-square representation} of the word $w$. For instance, in the word $\overline{a \stackrel{ab}{\sim} \; \stackrel{bc}{\sim}}\ \overline{a \stackrel{ab}{\sim} b \stackrel{bc}{\sim} c}\ a$ we have couples:\\
\indent $(a, \stackrel{ab}{\sim})$,
$(\stackrel{ab}{\sim}, \stackrel{bc}{\sim})$,
$(\stackrel{bc}{\sim}, a)$,
$(a, \stackrel{ab}{\sim})$,
$(\stackrel{ab}{\sim}, b)$,
$(b, \stackrel{bc}{\sim})$,
$(\stackrel{bc}{\sim}, c)$, and
$(c, a)$.\\
The corresponding circle-square representation of this word is: $\Circle \Circle \Square \Circle \Circle \Circle \Circle \Square$.

For $w \in \Sigma^{\ge 2}$, let us define $\psi(w)$ to be the circle-square representation of $w$. First, note that $|\psi(w)| = |w| - 1$. Second, for each $1 \le i < j \le n: \psi(w[i \ldots j]) = \psi(w)[i \ldots j - 1]$.

The circle-square representation of the jump and recovery operation is:\\
\indent \indent (1) \ circle-square jump: $\Square \underline{\Circle} \Circle \dashv \Square \Square \Square \Circle$,\\
\indent \indent (2) \ circle-square recovery: $\Circle \underline{\Square} \Square \dashv \Circle \Circle \Circle \Square$.

The corresponding instructions for these operations are:\\
\indent \indent (1) \ circle-square jump instruction: $(\Square, \underline{\Circle} \to \Square \Square, \Circle)$,\\
\indent \indent (2) \ circle-square recovery instruction : $(\Circle, \underline{\Square} \to \Circle \Circle, \Square)$.

\begin{lemma}\label{lemma:circle-square-jump-and-recover}
Suppose $w \in \Sigma^*$.\\
\indent (1) \ If $\sigma \in \{a, \tilde{a}\}$ and $w' = \sigma x y z \in \Sigma^4$ is a subword of $w$, then there exists a jump instruction $(\sigma x, \lambda \to \sigma', yz)$ applicable to the subword $w'$ of $w$ if and only if a circle-square jump instruction $(\Square, \underline{\Circle} \to \Square \Square, \Circle)$ can be applied to $\psi(w')$, i.e. if $\psi(w') = \Square \Circle \Circle$.\\
\indent (2) \ If $w' = x y z d \in \Sigma^4$ is a subword of $w$, then there exists a recovery instruction $(xy, \lambda \to \stackrel{uv}{\sim}, zd)$ applicable to the subword $w'$ of $w$ if and only if a circle-square recover instruction $(\Circle, \underline{\Square} \to \Circle \Circle, \Square)$ can be applied to $\psi(w')$, i.e. if $\psi(w') = \Circle \Square \Square$.
\end{lemma}

\begin{proof}
The proof is an immediate consequence of the definition of the jump and recovery operation.
\end{proof}

%Thus if we are not interested in concrete symbols of words, but only in which couples of
%consecutive symbols satisfy the ether property and which do not,
%we can restrict ourselves to the circle-square representation of words and
%the aforementioned circle-square jump and recovery instructions.

Thus if the concrete symbols of a word are not important but we study which pairs of consecutive symbols satisfy the ether property, we can restrict ourselves to the circle-square representation of words and
the aforementioned circle-square jump and recovery instructions.

\begin{example}
If we rewrite Examples \ref{example:spread} and \ref{example:recover} in a circle-square representation,
then the signal spreading part from Example \ref{example:spread} is:
$$
\begin{array}{l}
(1) \quad \underline{\Circle} \Circle \Circle \Circle \dashv\\
(2) \quad \Square \Square \underline{\Circle} \Circle \Circle \dashv\\
(3) \quad \Square \Square \Square \Square \underline{\Circle} \Circle \dashv\\
(4) \quad \Square \Square \Square \Square \Square \Square \underline{\Circle} \dashv\\
(5) \quad \Square \Square \Square \Square \Square \Square \Square \Square
\end{array}
$$
and the recovery part from Example \ref{example:recover} is:
$$
\begin{array}{l}
(5) \quad \underline{\Square} \Square \Square \Square \Square \Square \Square \Square \dashv\\
(6) \quad \Circle \Circle \underline{\Square} \Square \Square \Square \Square \Square \Square \dashv\\
(7) \quad \Circle \Circle \Circle \Circle \underline{\Square} \Square \Square \Square \Square \Square \dashv\\
(8) \quad \Circle \Circle \Circle \Circle \Circle \Circle \underline{\Square} \Square \Square \Square \Square \dashv\\
(9) \quad \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \underline{\Square} \Square \Square \Square \dashv\\
(10) \quad \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \underline{\Square} \Square \Square \dashv\\
(11) \quad \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \underline{\Square} \Square \dashv\\
(12) \quad \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \underline{\Square} \dashv\\
(13) \quad \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle \Circle\;.
\end{array}
$$
\end{example}

Consider a $\kCRS[1]$ $R = (\{\Circle, \Square\}, \Omega)$ with the following instructions:
$$\begin{array}{l}
(0) \quad (\cent, \lambda \to \Circle^m, \$),\\
(1) \quad (\{\cent, \Square\}, \underline{\Circle} \to \Square \Square, \{\Circle, \$\}),\\
(2) \quad (\{\cent, \Circle\}, \underline{\Square} \to \Circle \Circle, \{\Square, \$\}).
\end{array}
$$
According to Theorem \ref{theorem:circle-square-cap-theorem}, $L(R^D) \cap \{\Circle\}^+ = \{\Circle^{x} \mid x = 4^l m, \ l = 0, 1, 2, \ldots\}$, where $m \ge 1$ and $\alpha = \beta = 2$.

Note that the circle-square representation of the word $w_0 = \overline{X \stackrel{Xe}{\sim} \; \stackrel{ee}{\sim} \; \stackrel{eY}{\sim} Y}$ is $\Circle^4$ and the circle-square representation of $L_{ether}$ is the language $\{\Circle\}^+$. Let $L$ denote the language generated from the word $w_0$ by continual sending a signal (represented by the symbols $a$ and $\tilde{a}$) from $X$ to $Y$, as it was described above  in Example \ref{example:spread} and Example \ref{example:recover}. It is easy to see that the circle-square representation of $L$ is equal to $L(R^D)$ for $m=4$. Thus the circle-square representation of $L \cap L_{ether}$ is $L(R^D)\cap \{\Circle\}^+$, which is equal to $\{\Circle^{x} \mid x = 4^k, \ k = 1, 2, 3, \ldots\}$. This implies that the language $L \cap L_{ether}$ contains only words of length $4^k + 1$, and for each $k \ge 1$ at least one word.

The approach described so far can be generalized in many ways. For instance, we can have more membranes, more kinds of signals, and we can also send these signals in both directions. If we send signals in both directions, then we have to ensure that in every transfer medium the communication flows only in one direction at any given time.

On the other hand, this approach allows us to construct a $\kclRA[2]$ accepting a non-context-free language by using only $6$-letter alphabet. First, the signal is represented by symbols $a$ and $\tilde{a}$, and these two symbols are the only basic symbols. Thus $\Lambda = \{a, \tilde{a}\}$ and the corresponding alphabet $\Sigma = \Lambda \cup \wave(\Lambda)$ contains exactly $2 + 4 = 6$ letters. Second, we do not use special symbols for membranes, i.e. $X$ and $Y$. Instead of $X$ we can use $a$ and instead of $Y$ we can use $a$, as well. The (dual) instructions of the resulting automaton simulate continual sending of the signal from the first symbol $a$ to the last symbol $a$, and the starting word is $w_0 = a \stackrel{aa}{\sim} \; \stackrel{aa}{\sim} \; \stackrel{aa}{\sim} a$.

\subsection{$\kclRA[3]$ Recognizing a non-Context-Free Language}\label{3clRA-non-CFL}

The question, whether there exists a $\kclRA[2]$ with alphabet consisting of less than six letters recognizing a non-context-free language, remains open. However, there exists a $\kclRA[3]$ accepting a non-context-free language on alphabet $\Sigma = \{a, b\}$. Also the idea of this automaton is based on sending a signal through the ether. Let us consider the following productions:
\begin{eqnarray*}
& & aaabbbaaabbb \dashv_M\\
& & a\underline{b}aabbbaaabbb \dashv_M
aba\underline{b}abbbaaabbb \dashv_M\\
& & ababab\underline{a}bbaaabbb \dashv_M
abababab\underline{a}baaabbb \dashv_M\\
& & abababababa\underline{b}aabbb \dashv_M
ababababababa\underline{b}abbb \dashv_M\\
& & abababababababab\underline{a}bb \dashv_M
ababababababababab\underline{a}b \dashv_M\\
& & \underline{aa}abababababababababab \dashv_M\\
& & aaa\underline{bb}bababababababababab \dashv_M\\
& & aaabbb\underline{aa}ababababababababab \dashv_M\\
& & aaabbbaaa\underline{bb}babababababababab \dashv_M\\
& & \ldots\\
& & (aaabbb)^{8}\underline{aa}abab \dashv_M\\
& & (aaabbb)^{8}aaa\underline{bb}bab \dashv_M\\
& & (aaabbb)^{8}aaabbb\underline{aa}ab \dashv_M\\
& & (aaabbb)^{8}aaabbbaaa\underline{bb}b\;.
\end{eqnarray*}
The corresponding $\kclRA[3]$ $M = (\Sigma, \Phi)$ has the following instructions:
$$
\begin{array}{ll}
a_1 = (bab, \underline{a}, b\$),\hspace{5em}    & c_1 = (\cent, \underline{aa}, aba),\\
a_2 = (bab, \underline{a}, baa) ,               & c_2 = (bbb, \underline{aa}, ab\$),\\
a_3 = (bab, \underline{a}, bb\$),               & c_3 = (bbb, \underline{aa}, aba),\\
a_4 = (bab, \underline{a}, bba),                & d_1 = (aaa, \underline{bb}, b\$),\\
b_1 = (\cent a, \underline{b}, aab),            & d_2 = (aaa, \underline{bb}, bab),\\
b_2 = (aba, \underline{b}, aab),                & e_1 = (\cent, \underline{aaabbbaaabbb}, \$).\\
b_3 = (aba, \underline{b}, abb),
\end{array}
$$
It is difficult to describe the language $L(M)$ directly. Therefore, we introduce the following circle-square representation: Suppose that we have $w = a_1 a_2 \ldots a_n \in \Sigma^*$. Consider the corresponding sequence of couples $(a_1, a_2)$, $(a_2, a_3)$, ..., $(a_{n-1}, a_n)$. Now if we replace each couple in this sequence with the symbol $\Circle$ or $\Square$ depending on whether the corresponding couple $(a_i, a_{i+1})$ is homogenous (i.e. $a_i = a_{i+1}$) or not, we get a \emph{circle-square representation} of the word $w$.

For $w \in \Sigma^{\ge 2}$, we define $\chi(w)$ to be the circle-square representation of $w$. Of course, $|\chi(w)| = |w| - 1$, and for each $1 \le i < j \le n: \chi(w[i \ldots j]) = \chi(w)[i \ldots j - 1]$.

The circle-square representation of productions defining the automaton $M$ is:
\begin{eqnarray*}
& & \underline{\Circle} \Circle \Square \Circle \Circle \Square \Circle \Circle
\Square \Circle \Circle \dashv\\
& & \Square \Square \underline{\Circle} \Square \Circle \Circle \Square \Circle
\Circle \Square \Circle \Circle \dashv\\
& & \Square \Square \Square \Square \Square \underline{\Circle} \Circle \Square
\Circle \Circle \Square \Circle \Circle \dashv\\
& & \Square \Square \Square \Square \Square \Square \Square \underline{\Circle}
\Square \Circle \Circle \Square \Circle \Circle \dashv\\
& & \Square \Square \Square \Square \Square \Square \Square \Square \Square \Square
\underline{\Circle} \Circle \Square \Circle \Circle \dashv\\
& & \Square \Square \Square \Square \Square \Square \Square \Square \Square \Square
\Square \Square \underline{\Circle} \Square \Circle \Circle \dashv\\
& & \Square \Square \Square \Square \Square \Square \Square \Square \Square \Square
\Square \Square \Square \Square \Square \underline{\Circle} \Circle \dashv\\
& & \Square \Square \Square \Square \Square \Square \Square \Square \Square \Square
\Square \Square \Square \Square \Square \Square \Square \underline{\Circle} \dashv\\
& & \Square \Square \Square \Square \Square \Square \Square \Square \Square \Square
\Square \Square \Square \Square \Square \Square \Square \Square \Square \dashv\\
& & \Circle \Circle \underline{\Square} \Square \Square \Square \Square \Square \Square \Square \Square \Square
\Square \Square \Square \Square \Square \Square \Square \Square \Square \dashv\\
& & \Circle \Circle \Square \Circle \Circle \underline{\Square} \Square \Square \Square \Square \Square \Square \Square \Square
\Square \Square \Square \Square \Square \Square \Square \Square \Square \dashv\\
& & \Circle \Circle \Square \Circle \Circle \Square \Circle \Circle \underline{\Square} \Square \Square \Square \Square \Square \Square \Square
\Square \Square \Square \Square \Square \Square \Square \Square \Square \dashv\\
& & \Circle \Circle \Square \Circle \Circle \Square \Circle \Circle \Square \Circle \Circle \underline{\Square} \Square \Square \Square \Square \Square \Square
\Square \Square \Square \Square \Square \Square \Square \Square \Square \dashv\\
& & \ldots\\
& & (\Circle \Circle \Square)^{16} \Circle \Circle \underline{\Square} \Square \Square \dashv\\
& & (\Circle \Circle \Square)^{16} \Circle \Circle \Square \Circle \Circle \underline{\Square} \Square \dashv\\
& & (\Circle \Circle \Square)^{16} \Circle \Circle \Square \Circle \Circle \Square \Circle \Circle \underline{\Square} \dashv\\
& & (\Circle \Circle \Square)^{16} \Circle \Circle \Square \Circle \Circle \Square \Circle \Circle \Square \Circle \Circle\enspace.
\end{eqnarray*}

Consider a $\kCRS[2]$ $R = (\{\Circle, \Square\}, \Omega)$ with instructions:
$$
\begin{array}{ll}
X_0 = (\cent, \underline{\Circle} \to \Square \Square, \Circle \Square),\hspace{5em} &
Y_0 = (\cent, \lambda \to \Circle \Circle, \Square \Square),\\
X_1 = (\Square \Square, \underline{\Circle} \to \Square \Square, \Circle \Square), &
Y_1 = (\Circle \Circle, \underline{\Square} \to \Square \Circle \Circle, \Square \Square),\\
X_2 = (\Square \Square, \underline{\Circle} \to \Square \Square, \Square \Circle), &
Y_2 = (\Circle \Circle, \underline{\Square} \to \Square \Circle \Circle, \Square \$),\\
X_3 = (\Square \Square, \underline{\Circle} \to \Square \Square, \Circle \$), &
Y_3 = (\Circle \Circle, \underline{\Square} \to \Square \Circle \Circle, \$),\\
X_4 = (\Square \Square, \underline{\Circle} \to \Square \Square, \$), &
Z_0 = (\cent, \lambda \to \Circle \Circle \Square \Circle \Circle \Square \Circle \Circle \Square \Circle \Circle, \$).\\
\end{array}
$$
Then, trivially for each instruction $\phi \in \Phi^D$, if $\phi$ can be applied to the word $w \in \Sigma^{\ge 2}$, then there exists $\omega \in \Omega$ which can be applied in the same way to the word $\chi(w)$. Thus the circle-square representation of the language $L(M)$ is included in $L(R^D)$.

To analyze the language $L(R^D)$ we first introduce several auxiliary definitions. An \emph{ether} is a word $w \in \{\Circle, \Square\}^+$ which starts and ends with $\Circle$, and if for some $1 < i < |w|$ we have $w[i] = \Square$, then $w[i-1] = w[i+1] = \Circle$, i.e. all squares are single. A \emph{factor} is a word $\Square^k w \neq \lambda$ such that $k \ge 0$ and $w$ is either $\lambda$, or an ether. For a word $w \in \{\Circle, \Square\}^+$, we define a \emph{factorization} $w = w_0 w_1 \ldots w_n$ recursively as: $w_n$ is the longest suffix of $w$ such that $w_n$ is a factor, and $w_0 w_1 \ldots w_{n-1}$ is the factorization of the rest of the word. Since the word $w_n$ is defined unambiguously, it is easy to see by induction that the factorization of $w$ is unique. For instance, the word $w = \Square \Square \Circle \Square \Circle \Circle \Square \Square \Square \Square \Square \Square \Circle \Square \Circle \Circle \Square \Circle \Circle$ has the factors: $w_0 = \Square \Square \Circle \Square \Circle \Circle$ and $w_1 = \Square \Square \Square \Square \Square \Square \Circle \Square \Circle \Circle \Square \Circle \Circle$. The weight of a factor $u$ is defined as $\psi(u) = |u|_{\Square} + 2 |u|_{\Circle}$.

\begin{lemma}\label{lemma:circle-square-3clRA}
Suppose that $w \in L(R^D)$ and $w = w_0 w_1 \ldots w_n$ is a factorization of $w$. Let us denote $a_i = \psi(w_i)$, for all $i = 0, 1, \ldots, n$. Then for some $l \ge 1$:
$$
5^0 a_0 + 5^1 a_1 + \ldots + 5^n a_n = 4 \times 5^{n+l} - 1\enspace.
$$
We call this sum the overall sum of the word $w$.
\end{lemma}

\begin{proof}
(By induction on the number of used instructions of $R$)

The first applied instruction is always
$Z_0 = (\cent, \lambda \to \Circle \Circle \Square \Circle \Circle \Square \Circle \Circle \Square \Circle \Circle, \$)$.
This instruction can never be used later.
The factorization of the resulting word $w = \Circle \Circle \Square \Circle \Circle \Square \Circle \Circle \Square \Circle \Circle$
is clearly $w = w_0$, thus $n = 0$, $a_0 = \psi(w_0) = \psi(w) = 19$ and $5^0 a_0 = 19 = 4 \times 5^{0+l} - 1$ for $l = 1$.

Suppose that $w = w_0 w_1 \ldots w_n$ is a factorization of $w$ generated so far by
circle-square instructions, $a_i = \psi(w_i)$, for all $i = 0, 1, \ldots, n$,
and $5^0 a_0 + 5^1 a_1 + \ldots + 5^n a_n = 4 \times 5^{n+l} - 1$ for some $l \ge 1$.
We consider several cases depending on the next applied instruction.

\begin{enumerate}
\item $X_0 = (\cent, \underline{\Circle} \to \Square \Square, \Circle \Square)$: can be applied only to a prefix $\underline{\Circle} \Circle \Square$ of $w_0$, i.e. $w_0 = \underline{\Circle} \Circle \Square u_0$. After application we get $w_0' = \Square \Square \Circle \Square u_0$ which is obviously a factor with the same weight as $w_0$.
\item $X_1 = (\Square \Square, \underline{\Circle} \to \Square \Square, \Circle \Square)$: can be applied only to a subword $u = \Square \Square \underline{\Circle} \Circle \Square$ of $w$. Apparently, $u$ is a subword of some factor $w_i$, i.e. $w_i = \Square^k \Square \Square \underline{\Circle} \Circle \Square u_i$, $k \ge 0$. After application we get $w_i' = \Square^k \Square \Square \Square \Square \Circle \Square u_i$ which is a factor with the same weight as $w_i$.
\item $X_2 = (\Square \Square, \underline{\Circle} \to \Square \Square, \Square \Circle)$: can be applied only to a subword $u = \Square \Square \underline{\Circle} \Square \Circle$ of $w$. Obviously, $u$ is a subword of some factor $w_i$, i.e. $w_i = \Square^k \Square \Square \underline{\Circle} \Square \Circle u_i$, $k \ge 0$. After application we obtain $w_i' = \Square^k \Square \Square \Square \Square \Square \Circle u_i$, which is a factor with the same weight as $w_i$.
\item $X_3 = (\Square \Square, \underline{\Circle} \to \Square \Square, \Circle \$)$: can be applied only to a suffix $\Square \Square \underline{\Circle} \Circle$ of $w_n$, i.e. $w_n = \Square^k \Square \Square \underline{\Circle} \Circle$, $k \ge 0$. After application we get $w_n' = \Square^k \Square \Square \Square \Square \Circle$, which is a factor with the same weight as $w_n$.
\item $X_4 = (\Square \Square, \underline{\Circle} \to \Square \Square, \$)$: can be applied only to a suffix $\Square \Square \underline{\Circle}$ of $w_n$, i.e. $w_n = \Square^k \Square \Square \underline{\Circle}$, $k \ge 0$. After application we obtain $w_n' = \Square^k \Square \Square \Square \Square$, which is a factor with the same weight as $w_n$.
\item $Y_0 = (\cent, \lambda \to \Circle \Circle, \Square \Square)$: can be applied only to a prefix of $w_0$, i.e. $w_0 = \Square \Square \Square^k e_0$, $k \ge 0$, $e_0$ is either $\lambda$, or the ether. After application we get a new factor $w_0' = \Circle \Circle$ with $a_0' = \psi(w_0') = 4$, i.e. the corresponding $w' = w_0' w_1' \ldots w_{n+1}'$, where $w_{i+1}' = w_i$, $a_{i+1}' = a_i$, for all $i = 0, 1, \ldots, n$. Thus the overall sum is $5^0 a_0' + 5^1 a_1' + \ldots + 5^{n+1} a_{n+1}' = 4 + 5 \times (4 \times 5^{n+l} - 1) = 4 \times 5^{(n+1)+l} - 1$.
\item $Y_1 = (\Circle \Circle, \underline{\Square} \to \Square \Circle \Circle, \Square \Square)$: can be applied to a subword $\Circle \Circle \underline{\Square} \Square \Square$ of $w$, i.e. to a prefix of some factor $w_{i+1} = \underline{\Square} \Square \Square u_{i+1}$, where $w_i = u_i \Circle \Circle$. After application we get factors: $w_i' = u_i \Circle \Circle \Square \Circle \Circle$ and $w_{i+1}' = \Square \Square u_{i+1}$. Obviously, $\psi(w_i) + 5 \times \psi(w_{i+1}) = \psi(w_i') + 5 \times \psi(w_{i+1}')$, because $\psi(w_i') - \psi(w_i) = 5 \times (\psi(w_{i+1}) - \psi(w_{i+1}')) = 5$. Since all other factors remain unchanged, the overall sum does not change.
\item $Y_2 = (\Circle \Circle, \underline{\Square} \to \Square \Circle \Circle, \Square \$)$: can be applied to a suffix $\Circle \Circle \underline{\Square} \Square$ of $w$, i.e. $n \ge 1$, $w_{n-1} = u_{n-1} \Circle \Circle$, and $w_n = \underline{\Square} \Square$. After application we obtain factors: $w_{n-1}' = u_{n-1} \Circle \Circle \Square \Circle \Circle$ and $w_n' = \Square$. Obviously, $\psi(w_{n-1}) + 5 \times \psi(w_n) = \psi(w_{n-1}') + 5 \times \psi(w_n')$, because $\psi(w_{n-1}') - \psi(w_{n-1}) = 5 \times (\psi(w_n) - \psi(w_n')) = 5$. Since all other factors remain unchanged, the overall sum does not change.
\item $Y_3 = (\Circle \Circle, \underline{\Square} \to \Square \Circle \Circle, \$)$: can be applied to a suffix $\Circle \Circle \underline{\Square}$ of $w$, i.e. $n \ge 1$, $w_{n-1} = u_{n-1} \Circle \Circle$, and $w_n = \underline{\Square}$. After application we get factor: $w_{n-1}' = u_{n-1} \Circle \Circle \Square \Circle \Circle$ and the last factor vanishes. Apparently, $\psi(w_{n-1}) + 5 \times \psi(w_n) = \psi(w_{n-1}')$, because $\psi(w_{n-1}') - \psi(w_{n-1}) = 5 \times \psi(w_n) = 5$. All other factors remain unchanged, thus if we denote $a_i' = \psi(w_i')$, for all $i = 0, 1, \ldots, n-1$, then $5^0 a_0' + 5^1 a_1' + \ldots + 5^{n-1} a_{n-1}' = 5^0 a_0 + 5^1 a_1 + \ldots + 5^{n-1} (a_{n-1} + 5a_n) = 4 \times 5^{n+l} - 1 = 4 \times 5^{(n-1)+(l+1)} - 1$.
\end{enumerate}
\end{proof}

Note that only the instruction $Y_0 = (\cent, \lambda \to \Circle \Circle, \Square \Square)$ increases the number of factors and only the instruction $Y_3 = (\Circle \Circle, \underline{\Square} \to \Square \Circle \Circle, \$)$ decreases the number of factors. Also note that only the instruction $Y_0$ increases the overall sum, and only the instruction $Y_3$ increases $l$. All remaining instructions neither change the number of factors, nor the overall sum.

\begin{lemma}
$L(M) \cap \{(ab)^n \mid n > 0\} = \{(ab)^n \mid n = 2 \times 5^l, l = 1, 2, \ldots \}$.
\end{lemma}

\begin{proof}
Suppose that $(ab)^m \in L(M)$ for some $m > 0$. The circle-square representation of $(ab)^m$ is $\Square^{2m - 1}$. Since $\Square^{2m - 1}$ is a factor, according to Lemma \ref{lemma:circle-square-3clRA} we get $2m - 1 = 4 \times 5^{n+l} - 1$, where $n = 0$ and $l \ge 1$. Thus $m = 2 \times 5^l$.

On the other hand, we have to show that for each $l \ge 1$, $(ab)^{2 \times 5^l} \in L(M)$. First observe:
$$
\begin{array}{l}
aaabbb (aaabbb)^m aaabbb \dashv_M\\
a\underline{b}aabbb (aaabbb)^m aaabbb \dashv_M
aba\underline{b}abbb (aaabbb)^m aaabbb \dashv_M\\
ababab\underline{a}bb (aaabbb)^m aaabbb \dashv_M
abababab\underline{a}b (aaabbb)^m aaabbb \dashv_M\\
\ldots \dashv_M ababababab (ababababab)^m aaabbb \dashv_M\\
ababababab (ababababab)^m a\underline{b}aabbb \dashv_M
ababababab (ababababab)^m aba\underline{b}abbb \dashv_M\\
ababababab (ababababab)^m ababab\underline{a}bb \dashv_M
ababababab (ababababab)^m abababab\underline{a}b = \\
(ab)^5 (ababababab)^m (ab)^5\enspace.
\end{array}
$$
Similarly:
$$
\begin{array}{l}
abab (ab)^n abab \dashv_M\\
\underline{aa}abab (ab)^n abab \dashv_M
aaa\underline{bb}bab (ab)^n abab \dashv_M\\
aaabbb\underline{aa}ab (ab)^n abab \dashv_M
aaabbbaaa\underline{bb}b (ab)^n abab \dashv_M\\
\ldots \dashv_M aaabbbaaabbb (aaabbb)^n abab \dashv_M\\
aaabbbaaabbb (aaabbb)^n \underline{aa}abab \dashv_M
aaabbbaaabbb (aaabbb)^n aaa\underline{bb}bab \dashv_M\\
aaabbbaaabbb (aaabbb)^n aaabbb\underline{aa}ab \dashv_M
aaabbbaaabbb (aaabbb)^n aaabbbaaa\underline{bb}b = \\
(aaabbb)^2 (aaabbb)^n (aaabbb)^2\enspace.
\end{array}
$$
Consequently, for $m = 0$ we get: $\lambda \dashv_M aaabbbaaabbb \dashv_M^* (ab)^{2 \times 5}$, and for each $l \ge 1$: $(ab)^{2 \times 5^l} \dashv_M^* (aaabbb)^{2 \times 5^l} \dashv_M^* (ababababab)^{2 \times 5^l} = (ab)^{2 \times 5^{l+1}}$. Thus for each $l \ge 1: (ab)^{2 \times 5^l} \in L(M)$.
\end{proof}

The following theorem summarizes the results from the previous Sections
\ref{se:clRAandReg}, \ref{4clRA-non-CFL}, \ref{1clRA}, \ref{2clRA-non-CFL}, and \ref{3clRA-non-CFL},
and compares the class of languages recognized by clearing restarting automata
with the class of context-free languages.

\begin{theorem} Let us consider $\kclRA$ working with an alphabet $\Sigma$.
\begin{enumerate}
    \item[a)]
        $\calL{\clRA} = \CFL$ for $|\Sigma| = 1$.
    \item[b)]
        $\calL{\kclRA[1]} \subset \CFL$ for an arbitrary alphabet $\Sigma$.
    \item[c)]
        $\calL{\kclRA[2]} - \CFL \not= \emptyset$ for $|\Sigma| \ge 6$.
    \item[d)]
        $\calL{\kclRA[3]} - \CFL \not= \emptyset$ for $|\Sigma| \ge 2$.
\end{enumerate}
\end{theorem}

\subsection{Membership Problem for $\clRA$}\label{clra_membership}

In this section we study the membership problem for clearing restarting automata. Although, in general, it is $\NP$-complete to decide whether $w \in L(M)$ for $\clRA$ $M = (\Sigma, \Phi)$, it is possible to decide whether $w \in L(M)$ in polynomial time if $M \in \kclRA[1]$. The membership problem for $\clRA$ is clearly in $\NP$ since $w \in L(M) \Leftrightarrow \exists n \le |w|, w_1, \ldots, w_n \in \Sigma^{\le |w|}, \phi_1, \ldots, \phi_n \in \Phi: w = w_1 \vdash_M^{(\phi_1)} w_2 \ldots w_n \vdash_M^{(\phi_n)} \lambda$. We prove the $\NP$-hardness by reducing from $3$-$\SAT$.

\begin{theorem}\label{theorem:clra_membership}
The membership problem for $\clRA$ is $\NP$-complete. It is decidable in polynomial time for $\kclRA[1]$.
\end{theorem}

\begin{proof}
According to Corollary \ref{corollary:1clra_cfl} we can transform any $\kclRA[1]$ $M$ to an equivalent context-free grammar $G$ in polynomial time. Therefore, we can also decide whether $w \in L(M)$ in polynomial time. For general $\clRA$, consider a $3$-$\SAT$ formula $\psi = \bigwedge_{i=1}^n C_i$, where clause $C_i = \ell_{i,1} \vee \ell_{i,2} \vee \ell_{i,3}$, and $\ell_{i,1}$, $\ell_{i,2}$, $\ell_{i,3}$ are literals having pairwise different variables, for all $i \in \{1, 2, \ldots, n\}$. Let $\Omega = \{a_1, a_2, \ldots, a_m\}$ be the set of all variables occurring in $\psi$. In the following, we will effectively construct (in polynomial time) a $\clRA$ $M = (\Sigma, \Phi)$ and a word $w$, such that the following holds: the formula $\psi$ is satisfiable if and only if $w \in L(M)$. Our alphabet $\Sigma$ will contain all symbols from $\Omega \cup \overline{\Omega}$, where $\overline{\Omega} = \{ \overline{a_i} \mid a_i \in \Omega \}$, and $\Omega \cap \overline{\Omega} = \emptyset$. In addition, $\Sigma$ will contain also the following two special symbols: $\sharp, \Square$. The word $w$ is:
$$w = \Square a_1 \overline{a_1} \ldots a_m \overline{a_m} \sharp \ell_{1,1} \ell_{1,2} \ell_{1,3} \Square \ldots \Square a_1 \overline{a_1} \ldots a_m \overline{a_m} \sharp \ell_{n,1} \ell_{n,2} \ell_{n,3} \Square.$$
It consists of $n$ blocks of the form $a_1 \overline{a_1} \ldots a_m \overline{a_m} \sharp \ell_{i,1} \ell_{i,2} \ell_{i,3}$ separated by $\Square$. In the left part of each block we encode the assignment $v: \Omega \to \{0, 1\}$ of the truth values to the variables $a_1, \ldots, a_m$. We use the following convention: If both $a_j \overline{a_j}$ are present in the left part of the block, the truth value of $a_j$ is not yet assigned. If only $a_j$ is present then $v(a_j) = 1$. If only $\overline{a_j}$ is present then $v(a_j) = 0$. The $\clRA$ $M$ will (nondeterministically) assign the truth values to the variables by deleting the ``unwanted'' variables from the left part of blocks. We need to make sure that the assignments are consistent across all blocks. Moreover, the $\clRA$ $M$ will be able to delete the whole block $\alpha \sharp \beta$ if $\beta$ is true under the assignment defined by the left part $\alpha$. The problem is that the $\clRA$ $M$ can have only polynomially many instructions. We cannot, for instance, use the following set of instructions: $\{ (\Square, \underline{\alpha \sharp \beta \Square}, \lambda) \mid v_{\alpha}(\beta) = 1, \text{ where } v_{\alpha} \text{ is an assignment defined by } \alpha\}$, because there is exponentially many words (assignments) $\alpha$ such that $v_{\alpha}(\beta) = 1$. In the following, we outline how to achieve both consistency of the assignments across all blocks and at most polynomially many instructions. The resulting $\clRA$ is intended to work as follows. For $j = 1, \ldots, m$:
\begin{enumerate}
\item[(1)] Assign the truth value to the variable $a_j$ in the leftmost block of the input word (by deleting either $a_j$ or $\overline{a_j}$).
\item[(2)] Propagate this assignment to all blocks (from left to right).
\item[(3)] Eliminate all blocks that are true under this new assignment.
\item[(4)] Eliminate the variable $a_j$ (or $\overline{a_j}$) from the prefix of all blocks.
\end{enumerate}
Note that at the beginning of the $j$-th phase the input word is of the form:
$$\Square a_j \overline{a_j} \ldots a_m \overline{a_m} \sharp \beta_1 \Square \ldots \Square a_j \overline{a_j} \ldots a_m \overline{a_m} \sharp \beta_k \Square.$$
The instructions for the steps (1) -- (4) are:
\begin{enumerate}
\item[(1)] $(\cent \Square,\ a_j,\ \overline{a_j} a_{j+1} \overline{a_{j+1}} \ldots a_m \overline{a_m} \sharp)$,\\
$(\cent \Square a_j,\ \overline{a_j},\ a_{j+1} \overline{a_{j+1}} \ldots a_m \overline{a_m} \sharp)$.
\item[(2)] $(\Square \overline{a_j} a_{j+1} \overline{a_{j+1}} \ldots a_m \overline{a_m} \sharp \beta \Square,\ a_j,\ \overline{a_j} a_{j+1} \overline{a_{j+1}} \ldots a_m \overline{a_m} \sharp)$,\\
$(\Square a_j a_{j+1} \overline{a_{j+1}} \ldots a_m \overline{a_m} \sharp \beta \Square a_j,\ \overline{a_j},\ a_{j+1} \overline{a_{j+1}} \ldots a_m \overline{a_m} \sharp)$.
\item[(3)] $(\Square,\ a_j a_{j+1} \overline{a_{j+1}} \ldots a_m \overline{a_m} \sharp \beta \Square,\ \lambda)$, if $a_j$ is in $\beta$,\\
$(\Square,\ \overline{a_j} a_{j+1} \overline{a_{j+1}} \ldots a_m \overline{a_m} \sharp \beta \Square,\ \lambda)$, if $\overline{a_j}$ is in $\beta$.
\item[(4)] $(\Square,\ a_j,\ a_{j+1} \overline{a_{j+1}} \ldots a_m \overline{a_m} \sharp)$,\\
$(\Square,\ \overline{a_j},\ a_{j+1} \overline{a_{j+1}} \ldots a_m \overline{a_m} \sharp)$.
\end{enumerate}
Let $\Phi$ be the set of above instructions for all $j = 1, \ldots, m$ and for all $\beta = \ell_{i,1} \ell_{i,2} \ell_{i,3}$, where $i = 1, \ldots, n$. In addition, let $(\cent, \Square, \$) \in \Phi$. It is easy to see that $\clRA$ $M = (\Sigma, \Phi)$ can be effectively constructed in polynomial time and has polynomial size w.r.t. $n + m$. Moreover, if $\psi$ is satisfiable then by following the steps (1) -- (4) the input word $w$ can be reduced to a single $\Square$, which can then be reduced to $\lambda$ by using the instruction $(\cent, \Square, \$)$, i.e. $w \in L(M)$. On the other hand, let us assume that $w \in L(M)$, i.e. $w \vdash_M^* \lambda$. Our goal is to prove that $\psi$ is satisfiable. Observe that the only way to delete a subword $\sharp \beta \Square = \sharp \ell_{i,1} \ell_{i,2} \ell_{i,3} \Square$ (corresponding to the clause $C_i = \ell_{i,1} \vee \ell_{i,2} \vee \ell_{i,3}$) from the input word is by using an instruction of type (3). Such instruction also unambiguously specifies an assignment to some variable $a_j$ from $C_i$ such that $C_i$ is true under this assignment. If we collect all assignments from all instructions of type (3) used in the reduction $w \vdash_M^* \lambda$ we get an assignment satisfying the whole formula $\psi$, provided that the collected assignments are consistent. It is easy to see that the collected assignments are consistent because assignments can be created only at the first block of the input word by using instructions of type (1). Other blocks can only copy the assignments from previous blocks by using instructions of type (2). Also, instructions of type (4) cannot create new assignments. They can only forget existing assignments.
\end{proof}

\subsection{Extensions of $\clRA$}\label{clra_extensions}

Almost all models studied in this thesis are extensions of clearing restarting automata. One interesting example is a \emph{subword-clearing restarting automaton} (\emph{$\sclRA$} for short), which is a $\CRS$ $M = (\Sigma, \Phi)$, where for each instruction  $\phi = (x, z \to t, y) \in \Phi$: $z \in \Sigma^+$ and $t$ is a subword of $z$, such that $|t| < |z|$. Subword-clearing restarting automata are strictly more powerful than clearing restarting automata, because the following language $L = \{a^n c b^n \mid n \ge 0\}$ (not recognized by $\clRA$) is recognized by the $\sclRA$ $M = (\{a, b, c\}, \Phi)$ with instructions $\Phi = \{(a, \underline{acb} \to c, b), (\cent, \underline{acb}, \$)\}$. On the other hand, not all context-free languages can be recognized by subword-clearing restarting automata. 

\begin{theorem}[\cite{C13}]
The language $L = \{ w w^R \mid w \in \Sigma^* \}$, $|\Sigma| \ge 2$, is not recognized by any $\sclRA$.
\end{theorem}

\begin{proof}
Suppose (for contradiction) that there exists a $\sclRA$ $M = (\Sigma, \Phi)$  recognizing the language $L = \{ w w^R \mid w \in \Sigma^* \}$, where $|\Sigma| \ge 2$. Consider any instruction $\phi = (x, z \to t, y) \in \Phi$ such that $x$ or $y$ does not contain a sentinel. There exists at least one such instruction, because otherwise the language $L(M)$ would be finite. (This observation follows easily from the fact that an instruction  $(\cent x, z \to t, y\$)$ can be applied only to the word $xzy$). Without loss of generality assume that the right context $y$ does not contain the sentinel $\$$, i.e., $y \in \Sigma^*$. Let $\bar{x} \in \Sigma^*$ denote the left context $x$ without the sentinel $\cent$, i.e., $x = \bar{x}$ if $x \in \Sigma^*$, otherwise $x = \cent \bar{x}$. It is easy to see, that $\bar{x} z y \alpha \vdash_M^{(\phi)} \bar{x} t y \alpha$ for any $\alpha \in \Sigma^*$. Pick any two different letters $a, b \in \Sigma$. Consider the words $w = \bar{x} z y a^{2n} b$ and $w' = \bar{x} t y a^{2n} b$, where $n = |\phi|$. The word $w' \cdot w'^R$ has the middle point between the two letters $b$, which is denoted here by the dot: $w' \cdot w'^R = \bar{x} t y a^n a^n b \cdot b a^n a^n (\bar{x} t y)^R$. Apparently, $w' w'^R \in L$. The word $w w'^R$ has the middle point somewhere in the underlined part: $\bar{x} z y a^n \underline{a^n} b b a^n a^n (\bar{x} t y)^R$, because $|t| < |z| \le n$. It is easy to see that $w w'^R \notin L$: if you go $n$ steps to the left from the middle point of the word $w w'^R$, you will see only the letter $a$. But if you go $n$ steps to the right from the middle point of the word $w w'^R$, you will see also the letter $b$. This is a contradiction to the error preserving property of $M$, because $w w'^R \vdash_M^{(\phi)} w' w'^R$, but $w w'^R \notin L$ and $w' w'^R \in L$.
\end{proof}

Other extensions (that use auxiliary symbols) are studied in Chapter \ref{chapter:crs_aux}. In this Chapter we focus solely on context rewriting systems without auxiliary symbols. As we will see in the following Section \ref{section:inference}, not using auxiliary symbols allows us to do grammatical inference of many different types of restricted context rewriting systems (including clearing and subword-clearing restarting automata).

\section{Grammatical Inference of Restricted $\CRS$}\label{section:inference}

Grammatical inference is concerned with finding a description (representation or model) of a language when given only some information about the language, e.g. some words of the language, the structure of the language, counter-examples or access to an oracle.
One common root of many of the formalizations used in grammatical inference is Gold's model of \emph{identification in the limit}. E. Mark Gold can be justly called a pioneer in this field thanks to his seminal paper on ``Language identification in the limit'' \cite{Gold67limit}. The question of how children learn languages was part of his motivation; however, his focus was on theoretical investigations.
In Gold's model, a language is a set of strings over some fixed finite alphabet. In the following, we will use the term \emph{target language} to refer to a language which has to be learned. 
Assume, for a moment, that there is some fixed system of representations of languages, called a \emph{hypothesis space}, such that at least one correct model for the target language is contained in that system. Gold considers a \emph{learner} to be an algorithmic device which is given examples, step by step, and which, in each of the infinitely many steps, returns a hypothesis. A learner is considered successful if after some step the learner returns the same hypothesis over and over again for all future steps and the hypothesis it converges to is a correct representation for the target language in the underlying hypothesis space.
Gold considers both \emph{learning from text}, i.\,e.\ the case when only positive examples (strings belonging to the target language) are available for the learner, as well as \emph{learning from informant}, i.\,e.\ the case when both positive and negative examples (strings labeled according to whether or not they belong to the target language) are available. % More formally, a \emph{text} of a language $L$ is an infinite sequence of strings that contains all strings of $L$. Alternatively, an \emph{informant} of a language $L$ is an infinite sequence containing all strings over the underlying alphabet such that each string is labeled according to whether or not it belongs to $L$.

When concerned about \emph{efficient} grammatical inference \cite{delaHiguera1997} the main definitions of polynomial inference have been proposed by Pitt and Angluin. In his seminal paper \cite{Pitt89} Pitt discusses different possible ideas as to what polynomial complexity for the problem of identification in the limit of \emph{deterministic finite automata} ($\DFA$) should be. %Since we have no control over the presentation of the examples, Pitt discards the possibility of time being polynomial in the size of the representation to be learnt. He also refuses polynomial update time because the exhaustive search strategy can perform just that by an implementation where "when you have no time left, just delay the treatment of some examples for later". Therefore, Pitt proposes another measure of complexity: 
Pitt proposes that for an identification algorithm to be polynomial it must have polynomial \emph{update time}, and also make a polynomial number of \emph{implicit errors} (in the size of the automaton). An implicit error is made when the current hypothesis does not agree with a new example. However, this definition is shown (by Pitt) to be very restrictive, in the sense that no superclasses of regular languages allow polynomial time inference.

Another model of learning has been proposed by Angluin \cite{Angluin1988} where the presentation of the language can be controlled by asking queries to an oracle. There are two types of queries: the \emph{membership queries} (a string is proposed to the oracle which returns its correct classification), and \emph{equivalence queries}, where a representation is proposed to the oracle, which either accepts it as a correct representation of the language to be inferred, or returns a counter-example: a string from the symmetrical difference of the proposed language and the target one. This is known as the \emph{MAT} model (\emph{minimally adequate teacher}). With time complexity depending on the size of the automaton to be inferred and the length of the longest counter-example returned by the oracle, Angluin proves that $\DFA$ can be identified in polynomial time with membership queries and equivalence queries. Angluin also proves that both of these queries are necessary: neither membership queries alone, nor equivalence queries alone allow polynomial inference. 

Since both of these models show that even $\DFA$ cannot be inferred in polynomial time (unless strong oracles are used), we follow another theoretical framework provided by Gold in \cite{Gold78}: he presented a model for identification from informant, where a sample of labeled strings $(S^+, S^-)$, with $S^+$ a set of positive instances, and $S^-$ a set of negative instances, is presented to the inference algorithm that must return a representation compatible with $(S^+, S^-)$. The further conditions are that for each language there exists a \emph{characteristic sample} with which the algorithm returns a correct representation, and this must be monotonous in the sense that if correctly labeled examples are added to the characteristic set, then the algorithm infers the same language. These conditions insure identification, and it is easy to see that a class of representations is identifiable in the limit from given data if and only if it is identifiable in the limit from a complete presentation of examples, provided that we do not consider any efficiency criteria.

Most of the work in grammatical inference has focused on the area of learning regular languages. Now there are several good algorithms to deal with the case of learning from an informant \cite{Cicchello2003}. On the other hand, moving up the Chomsky hierarchy gives rise to some difficult problems. Characteristic samples (needed for identification) may not be of polynomial size, and although context-free grammars can be identified, in the framework of active learning, from a minimum adequate teacher \cite{Angluin1987}, the equivalence queries for context-free grammars are not computable. In \cite{Clark2010} Alexander Clark emphasizes the importance of learnability of the representation classes of formal languages. He proposes that one way to build learnable representations is by making them \emph{objective} or \emph{empiricist}: the structure of the representation should be based on the structure of the language. He illustrates this approach with three classes corresponding to the lowest three levels of the Chomsky hierarchy. All these classes are efficiently learnable under suitable learning paradigms. In defining these representation classes the author follows a simple slogan: ``Put learnability first!'' It means that we should design representations from the ground to be learnable. Rather than defining a representation, and then defining a function from the representation to the language, we should start by defining the map from the language to the representation. The basic elements of such formalism, whether they are states in an automaton, or non-terminals in a phrase-structure grammar, must have a clear definition in terms of sets of strings. In the conclusive remarks the author suggests that the  representations, which are both efficiently learnable and capable of representing mildly context-sensitive languages seem to be good candidates for models of human linguistic competence. 

Another promising alternative to tackle the learning barriers is to consider models which do not use auxiliary elements at all. Typical representatives of this category are \emph{contextual grammars} in \cite{M69}, \emph{pure context-free grammars} in \cite{maurer1980pure}, and \emph{locally testable languages} in \cite{MR74,Zal72}. In this thesis, we use \emph{context rewriting systems} ($\CRS$) without auxiliary symbols as our model. Our approach is reminiscent of the \emph{delimited string-rewriting systems} ($\DSRS$) introduced in \cite{Eyraud2007}, and described in Section \ref{section:delimited-string-rewriting-systems}. The main difference between delimited string-rewriting systems and context rewriting systems is that delimited string-rewriting systems use a specific order relation over the set of all terms and rules in order to make always only one single rule eligible for application for any given input string. This makes them an efficient (often linear) parsing device for strings with the membership problem decidable in polynomial time. Additionally, LARS \cite{delaHiguera2010} infers confluent string-rewriting systems, so enforcing any rewriting strategy such as leftmost-rewriting makes no difference. Context rewriting systems, on the other hand, are nondeterministic. To test whether a word $w$ belongs to the language $L(M)$ accepted by a given $\CRS$ $M$, one has to check whether $w$ can be reduced to the empty word $\lambda$ by a sequence of applications of the instructions of $M$. If we assume that the instructions are length-reducing, then every such sequence has at most $|w|$ steps. But as there could be several instructions that are applicable to the same word, or there could be several places at which a given instruction can be applied, all such sequences must be checked. It would be much better if we could assume that each and every sequence of applications of instructions of $M$ reduces $w$ to $\lambda$, if $w \in L(M)$. In this case we could restrict ourselves only to leftmost sequences of reductions, and accordingly, membership in $L(M)$ would be decidable deterministically in time $O(|w|)$. We call such context rewriting systems \emph{$\lambda$-confluent} and show that the proposed learning algorithm can be used to identify $\lambda$-confluent context rewriting system in the limit from informant. Although $\lambda$-confluence can be useful in practical applications, in \cite{OM13} it has been shown that $\lambda$-confluence is not even recursively enumerable for very restricted context rewriting systems. Nevertheless, we will show that the proposed learning algorithm works in the limit even if we do not check the $\lambda$-confluence of the inferred model.

The proposed inference algorithm can work with many different types of restricted context-rewriting systems. For instance, we use clearing restarting automata to prove some lower bounds on the complexity of learning.

This Section has the following structure. In Subsection \ref{section:learning} we discuss the paradigm of polynomial identification, as defined in \cite{Gold78, delaHiguera1997}, and then in Subsection \ref{section:algorithm} we introduce our learning algorithm built on slightly relaxed learning paradigm.

\subsection{Learning}\label{section:learning}

We now turn to our learning problem. We follow the approach used in \cite{Eyraud2007}.

\begin{definition}[\cite{Eyraud2007}]\label{definition:identification1}
Let $\calLL$ be a class of languages represented by some class $\calM$ of models.
\begin{enumerate}
\item A \emph{sample} $S$ for a language $L \in \calLL$ is a pair $(S^+, S^-)$ of two finite sets $S^+, S^- \subseteq \Sigma^*$ such that if $w \in S^+$ then $w \in L$ and if $w \in S^-$ then $w \notin L$. The \emph{size} of $S$, denoted as $\size(S)$, is the sum of the lengths of all the strings in $S^+$, $S^-$. Formally, $\size(S) = \size(S^+) + \size(S^-)$, where $\size(T) = \sum_{w \in T} |w|$.

\item An $(\calLL, \calM)$-learning algorithm $\calA$ is a program that takes as input a sample and outputs a representation from $\calM$. The \emph{size} of a model $M \in \calM$, denoted as $\size(M)$, can be defined as a measure proportional to the number of bits that we need in order to represent the model $M$.
\end{enumerate}
\end{definition}

We use the paradigm of polynomial identification, as defined in \cite{Gold78, delaHiguera1997}. In this paradigm we require that the learning algorithm has a running time polynomial in the size of the data from which it has to learn from. Next we want the algorithm to converge in some way to a chosen target, ideally after having seen a polynomial number of examples only. As this constraint is usually too hard, we want the convergence to take place in the limit, i.\,e., after having seen a finite number of examples. The polynomial aspects are then taken into account by using the size of a minimal \emph{characteristic} sample, whose presence should ensure identification.

\begin{definition}[Polynomial Identification \cite{delaHiguera1997}]\label{definition:identification2}
A class $\calLL$ of languages is \emph{identifiable in polynomial time and data from informat} for a class $\calM$ of models if and only if there exists a $(\calLL, \calM)$-learning algorithm $\calA$ and two polynomials $\alpha(\cdot)$ and $\beta(\cdot)$ such that:
\begin{enumerate}
\item\label{polynomial1} Given a sample $S = (S^+, S^-)$ for $L \in \calLL$ of size $m$, $\calA$ returns a model (hypothesis) $M \in \calM$ in $O(\alpha(m))$ time and $M$ is \emph{consistent with} $S$, i.\,e.\ $S^+ \subseteq L(M)$ and $S^- \cap L(M) = \emptyset$.

\item\label{polynomial2} For each model $M \in \calM$ representing the language $L \in \calLL$, there exists a finite \emph{characteristic sample} $S_0 = (S_0^+, S_0^-)$ of size at most $O(\beta(\size(M)))$ and a model $N \in \calM$: $L(N) = L(M)$, such that, on all samples $S = (S^+, S^-)$ for $L$ that satisfy $S_0^+ \subseteq S^+$ and $S_0^- \subseteq S^-$, $\calA$ returns the model $N$.
\end{enumerate}
\end{definition}

Unfortunately, Definition \ref{definition:identification2} is too strict for our purposes. We will need to relax this definition in two ways. First, we drop the requirement on the size of the characteristic sample, since in some cases the characteristic sample can be exponentially large. Second, we relax the condition $\calLL = \calL{\calM}$ to $\calLL \subseteq \calL{\calM}$. In other words, our class of models $\calM$ will be more expressive than the class of languages $\calLL$ that we want to infer. This is a reasonable relaxation in the setting of inferring $\lambda$-confluent models, since we are not able to effectively represent the class of $\lambda$-confluent models ($\lambda$-confluence is, in most cases, not even recursively enumerable).

\begin{definition}[Relaxed Polynomial Identification]\label{definition:identification3}
Let $\calLL$ be a class of languages that is representable by some class $\calM$ of models, i.\,e.\  $\calLL \subseteq \calL{\calM}$. A class $\calLL$ of languages is \emph{identifiable in polynomial time (but not data) from informat} for a class $\calM$ of models if and only if there exists a $(\calLL, \calM)$-learning algorithm $\calA$ and a polynomial $\alpha(\cdot)$ such that:
\begin{enumerate}
\item\label{relaxed-polynomial1} Given a sample $S = (S^+, S^-)$ for $L \in \calLL$ of size $m$, $\calA$ returns a model (hypothesis) $M \in \calM$ in $O(\alpha(m))$ time and $M$ is \emph{consistent with} $S$, i.\,e.\ $S^+ \subseteq L(M)$ and $S^- \cap L(M) = \emptyset$.

\item\label{relaxed-polynomial2} For each model $M \in \calM$ representing the language $L \in \calLL$, there exists a finite \emph{characteristic sample} $S_0 = (S_0^+, S_0^-)$ and a model $N \in \calM$: $L(N) = L(M)$, such that, on all samples $S = (S^+, S^-)$ for $L$ that satisfy $S_0^+ \subseteq S^+$ and $S_0^- \subseteq S^-$, $\calA$ returns the model $N$.
\end{enumerate}
\end{definition}

\subsection{Learning Algorithm}\label{section:algorithm}

In this section we propose a general learning algorithm for inferring various restricted $\lambda$-confluent context rewriting systems in polynomial time (but not data) from \emph{informant}.

In the following, the term \emph{model} refers to any context rewriting system $M \in \calM$, where $\calM$ is a fixed class of context rewriting systems restricted according to Definition \ref{definition:restrictions} (our hypothesis space). Our focus will be on the \emph{polynomial identification} of the class of languages $\calLL = \calL{\lconcalM}$ as defined in Section \ref{section:learning}. Since in most cases we cannot effectively decide $\lambda$-confluence, it is not possible to construct a $(\calLL, \lconcalM)$-learning algorithm. Therefore, we design a $(\calLL, \leftcalM)$-learning algorithm $\calA$ such that both conditions of Definition \ref{definition:identification3} are satisfied. If $\calM$ allows instructions of unrestricted width then the Condition \ref{relaxed-polynomial1} of Definition \ref{definition:identification3} is easily satisfied by returning a model $M = (\Sigma, \Phi)$ with $\Phi = \{ (\cent, w \to \lambda, \$) \mid w \in S^+, w \neq \lambda \}$, when given a sample $S = (S^+, S^-)$. In this case $L(M) = S^+$ and, in addition, $M$ is confluent, and thus also $\lambda$-confluent. Unfortunately, as the language $L(M)$ is always finite, only finite languages can be learned in this way. We need a more sophisticated algorithm that is able to deal also with infinite languages. To this end we first present in Section \ref{section:restricted-learning} an auxiliary inference procedure (Algorithm \ref{algorithm:lambda-learning}) that tries to infer a model consistent with the given sample such that the width of the model is restricted from above by some constant. This restriction is useful because it leads to models that can generalize over the presented sample. Then, in Section \ref{section:unrestricted-learning} we use this auxiliary inference procedure as a component in our final learning algorithm.

There is one additional subtlety involved in our learning schema that we need to discuss -- one of the (optional) input parameters of our learning algorithm is the length of contexts $k$. This parameter, if specified, restricts the hypothesis space $\calM$ to the class $\kcalM$ (see Definition \ref{definition:restrictions} for details). The class  $\kcalM$ allows only instructions where the length of contexts is constrained to be \emph{exactly} $k$ letters long. The only exception is, of course, the case when the contexts contain a sentinel. In that case they can be shorter than $k$ letters. An informal intuition behind constraining the contexts is as follows. The \emph{context} $(x, y)$ used in the instruction $\phi = (x, z \to t, y)$ limits the applicability of the \emph{instruction-rule} $z \to t$, so that we can rewrite the word $z$ to $t$ only if $z$ is placed inside the context $(x, y)$. The longer the contexts $x$ and $y$ are the smaller is the ``chance'' that the instruction-rule $z \to t$ will be applied in a ``wrong'' context. The main motivation for constraining the contexts arises from the famous $n$-gram model, which is often used, e.g., in \emph{language modeling} or \emph{statistical machine translation} \cite{Jurafsky}. By changing the length of contexts $k$ we can influence the sparsity of the resulting model just as we can influence the sparsity of the $n$-gram model by specifying $n$. If we overestimate the length of contexts $k$, the resulting model may become too sparse and may not generalize well. On the other hand, underestimating $k$ can lead to overgeneralization and divergence, as our target language may not be in the class $\calL{\kcalM}$. It can be an interesting research direction to investigate some heuristics for choosing the right length of contexts. But from the perspective of the identification in the limit we only need to choose long enough contexts, because it can be easily shown that for any class $\calM$ of context rewriting systems restricted according to Definition \ref{definition:restrictions} the following inclusions hold: $\calL{\kcalM[0]} \subseteq \calL{\kcalM[1]} \subseteq \calL{\kcalM[2]} \subseteq \ldots$. Moreover, if we do not specify the length of contexts $k$, the proposed learning algorithm will figure it out in the limit.

\subsection{Learning With Restricted Width}\label{section:restricted-learning}

The problem we are interested in here can be best described as follows. Given a sample $S = (S^+, S^-)$ for some language $L \in \calLL$, we would like to find a model (hypothesis) $M \in \klcalM$ consistent with $S$ (i.\,e., $S^+ \subseteq L(M)$ and $S^- \cap L(M) = \emptyset$), where $k$ is the \emph{optional} length of contexts and $l \ge 1$ is the finite prescribed maximal width of instructions. We use the notation $k = \cdot$ if $k$ is not specified, and we assume that $S^+ \cap S^- = \emptyset$ and $\lambda \in S^+$. As $\klcalM$ is a finite class, we could try to enumerate all models $M \in \klcalM$ and return the first model $M$ consistent with $S$. This is, however, a bad idea, as there are, in general, double exponentially many models $M \in \klcalM$ with respect to the width $l$. This follows easily from the fact that there are, in general, exponentially many instructions that have the width bounded from above by $l$ and each model $M \in \klcalM$ can be specified as a subset of these instructions. Since we are interested in polynomial identification as described in Section \ref{section:learning}, we need to use a different approach. The following Algorithm \ref{algorithm:lambda-learning} is an auxiliary inference procedure that will be used as a core component in our learning algorithm.

\begin{algorithm}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\caption{Auxiliary inference procedure $\mathsf{Infer}_{\calM}(S, k, l)$}
\label{algorithm:lambda-learning}
%\DontPrintSemicolon
\LinesNumbered
\Input{Sample $S = (S^+, S^-)$ over $\Sigma$,
$S^+ \cap S^- = \emptyset, \lambda \in S^+$.\\
Length of contexts $k \ge 0$, or $k = \cdot$, if not specified.\\
Maximal width of instructions $l \ge 1$.}
\Output{A model $M \in \klcalM$ possibly not consistent with $S$.}
$\Phi \leftarrow \mathsf{Assumptions}
(S^+, k, l)$\label{algorithm:lambda-learning:assumptions}\;
\While{$\exists w_- \in S^-, w_+ \in S^+, \phi \in \Phi: w_- \vdash^{(\phi)} w_+$\label
{algorithm:lambda-learning:cycle1-start}}
{$\Phi \leftarrow \Phi \setminus \{\phi\}$\;\label{algorithm:lambda-learning:cycle1-end}}
\While{$\exists w_+ \in S^+, w_- \in S^-, \phi \in \Phi: w_+ \vdash^{(\phi)} w_-$\label
{algorithm:lambda-learning:cycle2-start}}
{$\Phi \leftarrow \Phi \setminus \{\phi\}$\;\label
{algorithm:lambda-learning:cycle2-end}}
\Return{Model $M$ with the set of instructions $\Phi$}\;
\end{algorithm}

First, in Step \ref{algorithm:lambda-learning:assumptions} the function $\mathsf{Assumptions}(S^+, k, l)$ returns some set of \emph{instruction candidates}. We may assume that every returned instruction $\phi = (x, z \to t, y) \in \Phi$ is a \emph{legal} instruction of the class $\klcalM$ (i.\,e., $\phi$ satisfies all \emph{local} restrictions of the class $\klcalM$). Let us assume, for a moment, that $\Phi$ already contains all instructions of a target $\lambda$-confluent model $H$. Then in Loop \ref{algorithm:lambda-learning:cycle1-start}--\ref{algorithm:lambda-learning:cycle1-end} we gradually remove all instructions that allow a reduction from a negative sample to a positive sample, i.\,e., instructions that violate the \emph{error preserving property} (Lemma \ref{lemma:error-preserving}). In Loop \ref{algorithm:lambda-learning:cycle2-start}--\ref{algorithm:lambda-learning:cycle2-end} we gradually remove all instructions that allow a reduction from a positive sample to a negative sample, i.\,e., instructions that violate the \emph{correctness preserving property} (Lemma \ref{lemma:correctness-preserving}). These filtered instructions are definitely not in the set of instructions of the target $\lambda$-confluent model $H$, therefore their removal will not cause any problem. However, consistency with the given sample $S$ is not guaranteed after Loop \ref{algorithm:lambda-learning:cycle1-start}--\ref{algorithm:lambda-learning:cycle1-end}. Similarly, it is not guaranteed that after Loop \ref{algorithm:lambda-learning:cycle2-start}--\ref{algorithm:lambda-learning:cycle2-end} the resulting model will be $\lambda$-confluent. The success of the above algorithm, therefore, depends both on the initial instruction candidates obtained in Step \ref{algorithm:lambda-learning:assumptions} and on the given sample $S$. Nevertheless, we will show that if we have a ``reasonable'' function $\mathsf{Assumptions}$, then there always exists a finite \emph{characteristic sample} $S_0 = (S_0^+, S_0^-)$ corresponding to the target $\lambda$-confluent model $H$.

The time complexity of Algorithm \ref{algorithm:lambda-learning} depends on the time complexity of the function $\mathsf{Assumptions}$ in Step \ref{algorithm:lambda-learning:assumptions}. As we will see below, there exist ``correct'' functions $\mathsf{Assumptions}$ (for any class $\calM$ of $\CRS$ restricted according to Definition \ref{definition:restrictions}) that run in polynomial time. If the function $\mathsf{Assumptions}$ runs in polynomial time, then also the size of the set $\Phi$ is polynomial (with respect to the size of the input) and therefore also Loops \ref{algorithm:lambda-learning:cycle1-start}--\ref{algorithm:lambda-learning:cycle1-end} and \ref{algorithm:lambda-learning:cycle2-start}--\ref{algorithm:lambda-learning:cycle2-end} run in polynomial time. 

In the following Definition \ref{definition:assumptions} we define what we mean by the term \emph{correct} function $\mathsf{Assumptions}$.

\begin{definition}[Correct assumptions]\label{definition:assumptions}
We call the function $\mathsf{Assumptions}$ \emph{correct} with respect to a class $\calM$ of context rewriting systems restricted according to Definition \ref{definition:restrictions}, if:
\begin{enumerate}
\item For every set $S^+ \subseteq \Sigma^*$, the set of instructions $\Phi = \mathsf{Assumptions}(S^+, k, l)$ is finite. Additionally, every instruction $\phi = (x, z \to t, y) \in \Phi$ is a \emph{legal} instruction of the class $\klcalM$ (i.\,e., $\phi$ satisfies all local restrictions of the class $\klcalM$).
\item For every $M = (\Sigma, \Phi) \in \klcalM$  with \emph{minimal set of instructions}, there exists a finite set $S_0^+ \subseteq L(M)$ such that for every $S^+ \supseteq S_0^+: \Phi \subseteq \mathsf{Assumptions}(S^+, k, l)$.
\end{enumerate}
\end{definition}

\begin{definition}[Monotone assumptions]\label{definition:monotone-assumptions}
We call the function $\mathsf{Assumptions}$ \emph{monotone} if:
\begin{enumerate}
\item For every $S_1^+ \subseteq S_2^+ \subseteq \Sigma^*:
\mathsf{Assumptions}(S_1^+, k, l) \subseteq \mathsf{Assumptions}(S_2^+, k, l)$, 
\item For every $l_1 \le l_2:
\mathsf{Assumptions}(S^+, k, l_1) \subseteq \mathsf{Assumptions}(S^+, k, l_2)$.
\end{enumerate}
\end{definition}


\begin{example}
The most trivial example of a correct function $\mathsf{Assumptions}$ is to return all possible legal instructions $\phi$ of the class $\klcalM$. The correctness and monotonicity follow easily. However, the number of returned instructions is, in general, exponentially large with respect to $l$, therefore such function would be of little interest in real applications.
\end{example}

\begin{example}\label{example:assumptions}
In this example we define two very natural examples of \emph{monotone} functions $\mathsf{Assumptions}$ that are correct with respect to any particular class $\calM$ of context rewriting systems restricted according to Definition \ref{definition:restrictions}.

\begin{enumerate}
\item $\mathsf{WeakAssumptions}_{\calM}(S^+, k, l) := \{ \phi = (x, z \to t, y) \mid \phi $ is a legal instruction of the class $\klcalM$ and $\exists w_1, w_2 \in S^+: xzy$ is a subword of $\cent w_1 \$$ and $xty$ is a subword of $\cent w_2 \$ \}$.

The basic intuition behind this function is the assumption that if both patterns $xzy$ and $xty$ occur in the set of positive samples as subwords, then it is justified to replace the word $z$ with $t$ in the context $(x, y)$. Note that if $k$ is specified then the more we increase the length of contexts $k$ the smaller (or equal) the number of such patterns we will find in the set of positive samples. The contexts serve here as a \emph{safety cushion} against the inference of incorrect instructions.

\item $\mathsf{StrongAssumptions}_{\calM}(S^+, k, l) := \{ \phi = (x, z \to t, y) \mid
\phi $ is a legal instruction of the class $\klcalM$ and
$\exists w_1, w_2 \in S^+: w_1 \vdash^{(\phi)} w_2\}$.

This condition is even more restrictive than the previous one.
It basically says that the instruction $\phi = (x, z \to t, y)$ is justified only in the
case when there are positive samples $w_1, w_2 \in S^+$ such that we can obtain $w_2$
from $w_1$ by using this instruction.
\end{enumerate}
\end{example}
Note that $\mathsf{WeakAssumptions}$ and $\mathsf{StrongAssumptions}$ are analogous to respectively \emph{$k, l$ substitutability} used in \cite{Yoshinaka2008} and \emph{$k, l$ local substitutability} introduced in \cite{Coste2012}.

\begin{lemma}
Both functions $\mathsf{Assumptions}$ from Example \ref{example:assumptions} are \emph{monotone} and \emph{correct} with respect to any fixed class $\calM$ of context rewriting systems restricted according to Definition \ref{definition:restrictions}.
\end{lemma}

\begin{proof}
Both functions from Example \ref{example:assumptions} are monotone and, in addition, for every set $S^+ \subseteq \Sigma^*$: $\mathsf{StrongAssumptions}_{\calM}(S^+,k,l) \subseteq \mathsf{WeakAssumptions}_{\calM}(S^+,k,l)$. Therefore, we only need to prove the correctness for the more restrictive function $\mathsf{StrongAssumptions}_{\calM}$. The correctness of the function $\mathsf{WeakAssumptions}_{\calM}$ will follow immediately. Let $M = (\Sigma, \Phi)$ be any $\CRS$ from $\klcalM$ with minimal set of instructions. The minimality of $M$ implies that for every instruction $\phi \in \Phi$ there exists a word $w_{\phi} \in L(M)$ such that the instruction $\phi$ is used in every accepting computation $w_{\phi} \vdash_M^* \lambda$ (otherwise we could remove the instruction $\phi$ from $M$.) Without loss of generality we may assume that the instruction $\phi$ must be used in the very first step of every accepting computation $w_{\phi} \vdash_M^* \lambda$. (This does not mean that $\phi$ is the only applicable instruction. There may also be some other instructions applicable to $w_{\phi}$, but they will definitely not lead to any accepting computation.) Let us fix, for every $\phi \in \Phi$, some accepting computation $w_{\phi} \vdash^{(\phi)} w_{\phi}' \vdash_M^* \lambda$. Now define $S_0^+ := \bigcup_{\phi \in \Phi} \{ w_{\phi}, w_{\phi}' \}$. Apparently $S_0^+ \subseteq L(M)$. Moreover, $\Phi \subseteq \mathsf{StrongAssumptions}_{\calM}(S_0^+, k, l)$. The correctness follows easily from the monotonicity of the function $\mathsf{StrongAssumptions}_{\calM}$.
\end{proof}

Algorithm \ref{algorithm:assumptions_weak} (Algorithm \ref{algorithm:assumptions_strong}, respectively) shows a possible implementation of the function $\mathsf{WeakAssumptions}_{\calM}$ ($\mathsf{StrongAssumptions}_{\calM}$, respectively). Both of these algorithms have polynomial time complexity with respect to size of the input $S^+$, because there are at most quadratically many subwords in the set of positive samples (with respect to the $\size(S^+)$). In both of these algorithms we use the variable $\mathsf{Map}$ as a dictionary-like data structure (e.g., hash table).

\begin{algorithm}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\caption{Implementation of $\mathsf{WeakAssumptions}_{\calM}(S^+, k, l)$}
\label{algorithm:assumptions_weak}
%\DontPrintSemicolon
\LinesNumbered
\Input{Set of positive samples $S^+$ over $\Sigma$, $\lambda \in S^+$.\\
Length of contexts $k \ge 0$, or $k = \cdot$, if not specified.\\
Maximal width of instructions $l \ge 1$.}
\Output{A set of legal instructions $\Phi$ of the class $\klcalM$.}
$\Phi \leftarrow \emptyset$\;
$Map \leftarrow \emptyset$\;
\ForEach{$w_+ \in S^+$ {\bf and each}
$\alpha xzy \beta = \cent w_+ \$$ such that
$x \in LC_k$, $y \in RC_k$, $|xzy| \le l$\label
{algorithm:assumptions_weak:cycle1}}
{
$Map[(x, y)] \leftarrow Map[(x, y)] \cup \{z\}$\;
}
\ForEach{$x, y, z, t$, such that $z \neq t$ and
$z, t \in Map[(x,y)]$\label
{algorithm:assumptions_cl1:cycle2}}
{
\If{$\phi = (x, z \to t, y)$ is a legal instruction of $\klcalM$}
{$\Phi \leftarrow \Phi \cup \{ (x, z \to t, y) \}$\;}
}
\Return{$\Phi$}\;
\end{algorithm}

\begin{algorithm}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\caption{Implementation of $\mathsf{StrongAssumptions}_{\calM}(S^+, k, l)$}
\label{algorithm:assumptions_strong}
%\DontPrintSemicolon
\LinesNumbered
\Input{Set of positive samples $S^+$ over $\Sigma$, $\lambda \in S^+$.\\
Length of contexts $k \ge 0$, or $k = \cdot$, if not specified.\\
Maximal width of instructions $l \ge 1$.}
\Output{A set of legal instructions $\Phi$ of the class $\klcalM$.}
$\Phi \leftarrow \emptyset$\;
$Map \leftarrow \emptyset$\;
\ForEach{$w_+ \in S^+$ {\bf and each}
$\alpha xty \beta = \cent w_+ \$$ such that
$x \in LC_k$, $y \in RC_k$, $|xty| \le l$\label
{algorithm:assumptions_strong:cycle1}}
{
$Map[(x, y)] \leftarrow Map[(x, y)] \cup \{t\}$\;
}
$DS^+ \leftarrow \cent \cdot S^+ \cdot \$ = \{ \cent w_+ \$ \mid w_+ \in S^+ \}$\;
\ForEach{$w_+ \in S^+$ {\bf and each}
$\alpha xzy \beta = \cent w_+ \$$ such that
$x \in LC_k$, $y \in RC_k$, $|xzy| \le l$\label
{algorithm:assumptions_strong:cycle2}}
{
\ForEach{$t \in Map[(x, y)], t \neq z$}
{
\If{$\phi = (x, z \to t, y)$ is a legal instruction of $\klcalM$ {\bf and}
 $\alpha xty \beta \in DS^+$}
{$\Phi \leftarrow \Phi \cup \{ (x, z \to t, y) \}$\;}
}
}
\Return{$\Phi$}\;
\end{algorithm}

In the following Example \ref{example:inference} we illustrate how Algorithm \ref{algorithm:lambda-learning} behaves when used on clearing restarting automata as the underlying class of models. We use the parameters $k = 1$ and $l = 6$.

\begin{example}[\cite{C12}]\label{example:inference}
Consider the class of clearing restarting automata ($\clRA$) and the function $ \mathsf{WeakAssumptions}_{\clRA}$ (from Example \ref{example:assumptions}). Imagine that our goal is to infer a model for the language $L = \{ a^n b^n \mid n \ge 0 \}$. Let us try $S^+ = \{\lambda, ab, aabb\}$. First, we would like to estimate the set $\Phi = \mathsf{WeakAssumptions}_{\clRA}(S^+, l, k)$  for $k = 1$ and $l = 6$. The set of all subwords of the delimited positive samples $\cent S^+ \$$ is:
$SW^+ = \{\lambda$, $\cent$, $a$, $b$, $\$$, 
$\cent \$$, $\cent a$, $aa$, $ab$, $bb$, $b \$$, 
$\cent aa$, $\cent ab$, $aab$, $abb$, $ab \$$, $bb \$$, 
$\cent ab \$$, $\cent aab$, $aabb$, $abb \$$, 
$\cent aabb$, $aabb \$$, $\cent aabb \$ \}.$
An instruction $(x, z, y)$, where $x \in LC_1 = \{a, b, \cent\}$,
$y \in RC_1 = \{a, b, \$\}$, $|z| > 0$ and $|xzy| \le l$, is justified, according to the definition of $\mathsf{WeakAssumptions}_{\clRA}$, if and only if both $xzy, xy \in SW^+$. Thus, only the following reductions are justified:
$\cent \underline{a} a \vdash \cent a$,
$a \underline{a} b \vdash ab$,
$a \underline{b} b \vdash ab$,
$b \underline{b} \$ \vdash b\$$,
$\cent \underline{ab} \$ \vdash \cent \$$,
$a \underline{ab} b \vdash ab$,
$\cent \underline{aabb} \$ \vdash \cent \$$.
Therefore, $\mathsf{WeakAssumptions}_{\clRA}(S^+, l, k) = $
$\{(\cent, \underline{a}, a)$,
$(a, \underline{a}, b)$,
$(a, \underline{b}, b)$,
$(b, \underline{b}, \$)$,
$(\cent, \underline{ab}, \$)$,
$(a, \underline{ab}, b)$,
$(\cent, \underline{aabb}, \$) \}$.
Apparently, each of the following instructions can reduce a word in the language $L$ to a word outside the language $L$:
$(\cent, \underline{a}, a)$,
$(a, \underline{a}, b)$,
$(a, \underline{b}, b)$,
$(b, \underline{b}, \$)$.
We can remove them easily by taking $S^- = \{ aab, abb \}$. We do not need to add anything else to $S^+$. The inference procedure (Algorithm \ref{algorithm:lambda-learning}) $\mathsf{Infer}_{\clRA}((S^+, S^-), l, k)$ will correctly output the model $N = (\{a, b\}, $ 
$\{(\cent, \underline{ab}, \$)$,
$(a, \underline{ab}, b) $,
$(\cent, \underline{aabb}, \$) \})$.
\end{example}

In the following Theorem \ref{theorem:lambda-inference} we state our first positive result concerning the grammatical inference of restricted $\lambda$-confluent context rewriting systems.

\begin{theorem}\label{theorem:lambda-inference}
Let $\calM$ be a class of context rewriting systems restricted according to Definition \ref{definition:restrictions} and let function $\mathsf{Assumptions}$ be monotone and correct with respect to $\calM$. Then, for every $\lambda$-confluent model $M \in \klcalM$ there exists a finite \emph{characteristic sample} $S_0 = (S_0^+, S_0^-)$ and a $\lambda$-confluent model $N \in \klcalM$ equivalent to $M$ such that, on all samples $S = (S^+, S^-)$ for $L(M)$ that satisfy $S_0^+ \subseteq S^+$ and $S_0^- \subseteq S^-$, Algorithm \ref{algorithm:lambda-learning} $\mathsf{Infer}_{\calM}(S, k, l)$ returns $N$.
\end{theorem}

\begin{proof}
Let $M = (\Sigma, \Phi) \in \klcalM$. Without loss of generality we may assume that $M$ has a minimal set of instructions. According to Definition \ref{definition:assumptions}, there exists an $S_0^+ \subseteq L(M)$ such that for every $S^+ \supseteq S_0^+: \Phi \subseteq \mathsf{Assumptions}(S^+, k, l)$. Let us initialize the set of negative samples $S_0^-$ to the empty set. Let $\Theta$ denote the set of \emph{all legal instructions} $\phi = (x, z \to t, y)$ of the class $\klcalM$. (There are only finitely many such instructions, as $|\phi| \le l$.) We say that the instruction $\phi \in \Theta$ is \emph{bad} if there exist $w_- \notin L(M), w_+ \in L(M): (w_- \vdash^{(\phi)} w_+) \vee (w_+ \vdash^{(\phi)} w_-)$ and call the pair of words $(w_-, w_+)$ the \emph{witness} for the bad instruction $\phi$. We say that the instruction $\phi$ is \emph{disabled} by $(S_0^+, S_0^-)$ if there exist $w_- \in S_0^-, w_+ \in S_0^+: (w_- \vdash^{(\phi)} w_+) \vee (w_+ \vdash^{(\phi)} w_-)$. Now consider the following Algorithm \ref{algorithm:samples_extension}:

\begin{algorithm}\label{algorithm:extend}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\caption{Extension of Sample $S_0 = (S_0^+, S_0^-)$}
\label{algorithm:samples_extension}
%\DontPrintSemicolon
\LinesNumbered
\Input{Sample $S_0 = (S_0^+, S_0^-)$.}
\Output{Extended sample $S_0 = (S_0^+, S_0^-)$.}
%$S_0^- \leftarrow \emptyset$\;
\While{$\exists $ \emph{bad} instruction $ \phi \in \Theta $ such that $ \phi $ is not \emph{disabled} by $ (S_0^+, S_0^-)$\label{algorithm:extend:cycle}}
{
Let $w_- \notin L(M), w_+ \in L(M)$ be a \emph{witness} for $\phi$\;
$S_0^+ \leftarrow S_0^+ \cup \{ w_+ \}$\;
$S_0^- \leftarrow S_0^- \cup \{ w_- \}$\;
}
\end{algorithm}

Every added pair $w_+, w_-$ effectively disables at least one instruction from $\Theta$, so the whole procedure is definitely finite. Now consider any finite set of positive samples $S^+ \supseteq S_0^+$ and any finite set of negative samples $S^- \supseteq S_0^-$ consistent with $M$. If we run the learning Algorithm \ref{algorithm:lambda-learning} $\mathsf{Infer}_{\calM}(S^+, S^-, k, l)$, then in Step \ref{algorithm:lambda-learning:assumptions} we obtain some set of instructions including all instructions from $\Phi$. (This is guaranteed by the correctness of the function $\mathsf{Assumptions}$.) Note that no instruction from $\Phi$ is bad. In Loop \ref{algorithm:lambda-learning:cycle1-start}--\ref{algorithm:lambda-learning:cycle1-end} and Loop \ref{algorithm:lambda-learning:cycle2-start}--\ref{algorithm:lambda-learning:cycle2-end} Algorithm \ref{algorithm:lambda-learning} gradually removes all bad instructions. (This is because the sets $S_0^+$ and $S_0^-$ are constructed in such a way, that all bad instructions are disabled by $(S_0^+, S_0^-)$.) After these loops we are left only with correct instructions from $\Theta \supseteq \Phi$, so the resulting model is apparently equivalent to the model $M$. The resulting model is also $\lambda$-confluent. This is because if we take any $w \in L(M)$, then for every instruction $\phi$ of the resulting model and for every $w'$ such that $w \vdash^{(\phi)} w'$ we have $w' \in L(M)$. Otherwise the instruction $\phi$ would be bad. This means that the resulting model is \emph{correctness-preserving}, and therefore also $\lambda$-confluent. We may also assume that for all positive samples $S^+ \supseteq S_0^+$: $\mathsf{Assumptions}(S^+, k, l) = \mathsf{Assumptions}(S_0^+, k, l)$. This is because the function $\mathsf{Assumptions}$ is monotone and for all positive samples $S^+: \mathsf{Assumptions}(S^+, k, l) \subseteq \Theta$, where $\Theta$ is a finite set. This will guarantee us that the learning algorithm $\mathsf{Infer}_{\calM}(S^+, S^-, k, l)$ will return the same $\lambda$-confluent model for all samples $S = (S^+, S^-)$ for $L(M)$ that satisfy $S_0^+ \subseteq S^+$ and $S_0^- \subseteq S^-$.
\end{proof}

\begin{example}[\cite{C12}]
Consider the class of clearing restarting automata ($\clRA$) and the function $\mathsf{WeakAssumptions}_{\clRA}$ as in Example \ref{example:inference}. If we use $M = (\{a, b\}, \{(\cent, \underline{ab}, \$), (a, \underline{ab}, b) \})$ as our target model recognizing the language $L(M) = \{a^n b^n \mid n \ge 0 \}$ and the parameters $k = 1$, $l = 6$, then it can be shown that the following sets of positive and negative samples: 
$S_0^+ = \{ a^n b^n \mid 0 \le n \le 6 \}$ and
$S_0^- = \{ aab$, $abb$, $aaab$, $abbb$, $aaaab$, $aaabb$, $aabbb$, $abbbb$, 
$aaaaab$, $aaaabb$, $aabbbb$, $abbbbb$, $aaaaabb$, $aabbbbb$, 
$aaaaaabb$, $aabbbbbb \}$ represent the corresponding characteristic sample from Theorem \ref{theorem:lambda-inference}.
\end{example}

Theorem \ref{theorem:lambda-inference} only guarantees the existence of the characteristic sample $S_0 = (S_0^+, S_0^-)$. It does not provide any bounds on the size of the sample $S_0$. The size of the resulting characteristic sample $S_0$ depends not only on the target $\lambda$-confluent model $M \in \klcalM$, but also on the used function $\mathsf{Assumptions}$. In the following Example \ref{example:signals} we will see that, in general, the size of the smallest characteristic sample $S_0 = (S_0^+, S_0^-)$ can be exponentially large with respect to the size of the target model. We use clearing restarting automata and the function $\mathsf{StrongAssumptions}$ (from Example \ref{example:assumptions}) to prove this fact.

\begin{example}\label{example:signals}
In this example we construct a sequence of confluent $2$-clearing restarting automata: $M_0 = (\Sigma_0, \Phi_0), M_1 = (\Sigma_1, \Phi_1), \ldots$, such that for all $i \in \{0, 1, 2, \ldots\}: \Sigma_i = \{a_0, a_1, \ldots, a_i\}$, and $\Phi_i \subseteq \Phi_{i+1}$. We prove that for each $i \in \{0, 1, 2, \ldots\}$ the size of the automaton $M_i$ is polynomial with respect to $i$, and that there exists an instruction $\phi_i \in \Phi_i$ such that the smallest word $w_i \in L(M_i)$, for which the instruction $\phi_i$ is applicable, has an exponential  length with respect to $i$, and thus also with respect to the size of the automaton $M_i$. In constructing these automata we use a technique of sending signals from one  sentinel to the other (and vice versa), which was widely applied in \cite{CM10}.

The automaton $M_0 = (\Sigma_0, \Phi_0)$ accepts only one word: $a_0 a_0 a_0 a_0$, where $\Sigma_0 = \{ a_0 \}$ and $\Phi_0$ contains only one instruction $(\cent, a_0 a_0 a_0 a_0, \$)$.

The best way, how to describe other automata in the sequence is to show how they work in the reverse direction (i.\,e.\ how they generate words starting from the empty word by using the inverse of the rewriting relation $\dashv$ defined as $\vdash^{-1}$). The automaton $M_1$ just sends a signal $a_1$ from the left sentinel $\cent$ to the right sentinel $\$$ starting from the word $a_0 a_0 a_0 a_0$, as follows:
\\

\indent $\cent \lambda \$ \dashv \cent \underline{a_0 a_0 a_0 a_0} \$ \dashv 
\cent \underline{\mathbf{a_1}} a_0 a_0 a_0 a_0 \$ \dashv 
\cent \mathbf{a_1} a_0 \underline{\mathbf{a_1}} a_0 a_0 a_0 \$ \dashv
\cent \mathbf{a_1} a_0 \mathbf{a_1} a_0 \underline{\mathbf{a_1}} a_0 a_0 \$ \dashv $\\
\indent $\cent \mathbf{a_1} a_0 \mathbf{a_1} a_0 \mathbf{a_1} a_0 
\underline{\mathbf{a_1}} a_0 \$ \dashv 
\cent \mathbf{a_1} a_0 \mathbf{a_1} a_0 \mathbf{a_1} a_0 \mathbf{a_1} a_0 
\underline{\mathbf{a_1}} \$$.
\\

This can be achieved by the following set of instructions: $\Phi_1 = \{ (\cent, a_0 a_0 a_0 a_0, \$)$, $(\cent, \underline{a_1}, a_0 a_0)$, $(a_1 a_0, \underline{a_1}, a_0 a_0)$, $(a_1 a_0, \underline{a_1}, a_0 \$)$, $(a_1 a_0, \underline{a_1}, \$) \}$. It can be easily verified that the only words accepted by the automaton $M_1$ are the words shown in the above accepting computation. This is because if we proceed in the reverse direction, starting from the empty word, then we basically cannot get anything else than what we have in the above computation. This property also holds for all subsequent automata in the sequence.

The automaton $M_2 = (\Sigma_2, \Phi_2)$ is similar to $M_1$, except that this time it will send a signal $a_2$ from the right sentinel $\$$ to the left sentinel $\cent$. The reason why we want to send a signal in a reverse direction is that we want to preserve the nice property of having only one possible accepting computation. The automaton $M_2$ works as follows (it starts exactly where the previous automaton $M_1$ has ended):
\\

\indent $\cent a_1 a_0 a_1 a_0 a_1 a_0 a_1 a_0 a_1 \$ \dashv 
\cent a_1 a_0 a_1 a_0 a_1 a_0 a_1 a_0 a_1 \underline{\mathbf{a_2}} \$ \dashv$\\
\indent $\cent a_1 a_0 a_1 a_0 a_1 a_0 a_1 a_0 
\underline{\mathbf{a_2}} a_1 \mathbf{a_2} \$ \dashv
\cent a_1 a_0 a_1 a_0 a_1 a_0 a_1 
\underline{\mathbf{a_2}} a_0 \mathbf{a_2} a_1 \mathbf{a_2} \$ \dashv$\\
\indent $\cent a_1 a_0 a_1 a_0 a_1 a_0 
\underline{\mathbf{a_2}} a_1 \mathbf{a_2} a_0 \mathbf{a_2} a_1 \mathbf{a_2} \$ \dashv
\cent a_1 a_0 a_1 a_0 a_1 \underline{\mathbf{a_2}} a_0 \mathbf{a_2} a_1 
\mathbf{a_2} a_0 \mathbf{a_2} a_1 \mathbf{a_2} \$ \dashv$\\
\indent $\ldots$\\
\indent $\cent \mathbf{a_2} a_1 \mathbf{a_2} a_0 \mathbf{a_2} a_1 
\mathbf{a_2} a_0 \mathbf{a_2} a_1 \mathbf{a_2} a_0 \mathbf{a_2} a_1 
\mathbf{a_2} a_0 \mathbf{a_2} a_1 \mathbf{a_2} \$$
\\

To enable this kind of computation we only need to add the following instructions: $(\circ \circ, \underline{a_2}, \$)$, $(\circ \circ, \underline{a_2}, \circ a_2)$, $(\cent \circ, \underline{a_2}, \circ a_2)$, and $(\cent, \underline{a_2}, \circ a_2)$, where $\circ$ is a placeholder for any of the symbols from $\{ a_0, a_1 \}$. (Of course, different occurrences of the placeholder $\circ$ can be substituted by  different symbols.) As in the previous case, we have only one possible computation.

Now we can generalize the above construction also to other automata in the sequence. The automaton $M_i$, for $i > 0$, is obtained from the automaton $M_{i-1}$ as follows:
\begin{enumerate}
\item If $i$ is odd then the automaton $M_i$ will send a signal from the left sentinel $\cent$ to the right sentinel $\$$, thus, in order to obtain $\Phi_i$, we only need to add  the following instructions to $\Phi_{i-1}$: $(\cent, \underline{a_i}, \circ \circ)$, $(a_i \circ, \underline{a_i}, \circ \circ)$, $(a_i \circ, \underline{a_i}, \circ \$)$, and $(a_i \circ, \underline{a_i}, \$)$, where $\circ$ is a placeholder for any symbol from $\{ a_0, a_1, \ldots, a_{i-1} \}$.

\item If $i$ is even then the automaton $M_i$ will send a signal from the right sentinel $\$$ to the left sentinel $\cent$, thus, in order to obtain $\Phi_i$, we only need to add  the following instructions to $\Phi_{i-1}$: $(\circ \circ, \underline{a_i}, \$)$, $(\circ \circ, \underline{a_i}, \circ a_i)$, $(\cent \circ, \underline{a_i}, \circ a_i)$, and $(\cent, \underline{a_i}, \circ a_i)$, where $\circ$ is a placeholder for any symbol from $\{ a_0, a_1, \ldots, a_{i-1} \}$.
\end{enumerate}

First observe that the size of the automaton $M_i$ is polynomial with respect to $i$. This can be easily proved inductively by using a simple observation that we add only $O(i^3)$ new instructions to $\Phi_{i-1}$ when constructing $\Phi_i$.

Now consider any $i > 0$. If $i$ is even then let us take the instruction $\phi_i = (\cent, \underline{a_i}, a_{i-1} a_{i-2})$. This instruction can be applied only after the previous signal $a_{i-1}$ has arrived to the left sentinel $\cent$. In other words, it can be applied only to the longest word in $L(M_{i-1})$. However, the length of the longest word in $L(M_j)$ is exponential with respect to $j$, for all $j \ge 0$, because every time the signal traverses from one end to the other, the length of the resulting word more than doubles.

In the case when $i$ is odd we can take the instruction $\phi_i = (a_{i-2} a_{i-1}, \underline{a_i}, \$)$, which can be applied only after the previous signal $a_{i-1}$ has arrived to the right sentinel $\$$.

Additionally, every $M_i$ is confluent, as there is always only one accepting computation.
\end{example}

The above example clearly shows that sometimes we need to consider an exponentially large set (i.\,e.\ a set that has an exponentially large binary representation) of positive samples $S^+$ in order to obtain all instructions of the target model. The argument is based on the use of the function $\mathsf{StrongAssumptions}$ (from Example \ref{example:assumptions}) combined with the class of confluent clearing restarting automata. This naturally gives rise to an open question, whether this phenomenon also applies to other functions and other classes of restricted context rewriting systems.

As you can see, our auxiliary inference procedure (Algorithm \ref{algorithm:lambda-learning}) is relatively simple and straightforward, and, in addition, it sometimes fails to return a consistent model with the given input sample. This naturally leads to the question of whether one can design a more sophisticated algorithm that can, for instance, always return a model with the restricted maximal width that is consistent with the given set of positive and negative samples. It turns out that there is little hope in finding such algorithm, because the task of finding a clearing restarting automaton consistent with a given set of positive and negative samples is $\NP$-hard, provided there is an upper bound on the width of its instructions.

\begin{theorem}[\cite{C12}]\label{theorem:0clrainference}
Let $l \ge 2$ be a fixed integer. It is $\NP$-complete to decide whether there exists a $\zlclRA$ $M = (\Sigma, \Phi)$ consistent with a given sample $S = (S^+, S^-)$, $S^+ \cap S^- = \emptyset$, $\lambda \in S^+$.
\end{theorem}

\begin{proof}
Consider a $3$-$\SAT$ formula $\psi = \bigwedge_{i=1}^n C_i$, where clause $C_i = \ell_{i,1} \vee \ell_{i,2} \vee \ell_{i,3}$, and $\ell_{i,1}$, $\ell_{i,2}$,  $\ell_{i,3}$ are literals having pairwise different variables, for all  $i = 1, 2, \ldots, n$. Let $\Omega = \{a_1, a_2, \ldots, a_m\}$ be the set of all variables occurring in $\psi$. In the following, we will (effectively) construct a finite set of positive samples $S^+$ and a finite set of negative samples $S^-$, $S^+ \cap S^- = \emptyset$, $\lambda \in S^+$, such that the following holds: the formula $\psi$ is satisfiable if and only if there exists a $\zlclRA$ $M = (\Sigma, \Phi)$ consistent with $S^+$ and $S^-$.

Our alphabet will be $\Sigma = \Omega \cup \overline{\Omega}$, where $\overline{\Omega} = \{ \overline{a_i} \mid a_i \in \Omega \}$, and $\Omega \cap \overline{\Omega} = \emptyset$. First set $S^+ := \{ \lambda \}, S^- := \emptyset$. For each clause $C_i = \ell_{i,1} \vee \ell_{i,2} \vee \ell_{i,3}$ add the negative sample $w_{C_i}^- = \overline{\ell_{i,1}}\ \overline{\ell_{i,2}}\ \overline{\ell_{i,3}}$ to the set of negative samples $S^-$. (We define $\overline{\overline{a}} = a$ for all $a \in \Omega$). For each variable $a \in \Omega$ add the following positive samples: $w_0^+ = (a \overline{a})^l$, $w_1^+ = a^l$, $w_2^+ = \overline{a}^l$ to the set of positive samples $S^+$. And at last, for each $a \in \Omega$ add all words $w \in \{a, \overline{a}\}^{\le l}$, such that $|w|_a \ge 1$ and $|w|_{\overline{a}} \ge 1$, to the set of negative samples $S^-$. Note that, for fixed $l$, there is only finite number of such words for every $a \in \Sigma$. Thus the size of the constructed set of positive and negative samples is, in fact, linear with respect to the size of the formula $\psi$.

($\Rightarrow$)
Suppose that $\psi$ is satisfiable, i.e. there exists an assignment $v: \Omega \to \{0, 1\}$ such that $v^*(C_i) = 1$ for all $i \in \{1, 2, \ldots, n\}$, where $v^*$ denotes the natural extension of $v$ to formulas. We will construct a $\zlclRA$ $M = (\Sigma, \Phi)$ consistent with $S^+$ and $S^-$. Let $\Phi=\Phi_1 \cup \Phi_2$, where $\Phi_1 = \{ (\lambda, a, \lambda) \mid a \in \Omega: v(a) = 1 \} \cup \{ (\lambda, \overline{a}, \lambda) \mid a \in \Omega: v(a) = 0 \}$ and $\Phi_2 = \{ (\lambda, a^l, \lambda), (\lambda, \overline{a}^l, \lambda) \mid a \in \Omega \}$. It can be easily observed that, for each literal $\ell \in \Omega \cup \overline{\Omega}: \ell \vdash_M \lambda \Leftrightarrow v(\ell) = 1$, or equivalently: $\overline{\ell} \vdash_M \lambda \Leftrightarrow v(\ell) = 0$. Therefore, no negative sample $w_{C_i}^- = \overline{\ell_{i,1}}\ \overline{\ell_{i,2}}\ \overline{\ell_{i,3}}$, for $i = 1, 2, \ldots, n$, can be reduced to the empty word $\lambda$ by using the instructions from $\Phi_1$, because, otherwise, it would mean that $v(\ell_{i,1}) = v(\ell_{i,2}) = v(\ell_{i,3}) = 0$. As the literals used in $w_{C_i}^-$ have all pairwise different variables, no instruction from $\Phi_2$ can be applied to it. Therefore, the resulting automaton $M$ cannot reduce any negative sample of the form  $w_{C_i}^- = \overline{\ell_{i,1}}\ \overline{\ell_{i,2}}\ \overline{\ell_{i,3}}$ to the empty word $\lambda$. Moreover, all positive samples of the form $w_0^+ = (a \overline{a})^l$,  $w_1^+ = a^l$, $w_2^+ = \overline{a}^l$ can be reduced to the empty word $\lambda$. For each $a \in \Omega$ there is either the instruction $(\lambda, a, \lambda)$, or the instruction $(\lambda, \overline{a}, \lambda)$ in $\Phi_1$. Therefore, we can always reduce the positive sample $w_0^+ = (a \overline{a})^l$ either to the word $a^l$, or $\overline{a}^l$. After that, we can use one of the instructions in $\Phi_2$ to reduce it further to the empty word $\lambda$. Therefore, for each $a \in \Omega$: $(a \overline{a})^l \vdash_M^* \lambda$, and also trivially $a^l \vdash_M \lambda$, and $\overline{a}^l \vdash_M \lambda$. Finally, for each $a \in \Omega$ the word $w \in \{a, \overline{a}\}^{\le l}$, such that $|w|_a \ge 1$ and $|w|_{\overline{a}} \ge 1$, cannot be reduced to the empty word $\lambda$. This is because there is only one of the instructions: $(\lambda, a, \lambda)$, $(\lambda, \overline{a}, \lambda)$ available in $\Phi_1$, and we will never be able to use any of the instructions: $(\lambda, a^l, \lambda)$, $(\lambda, \overline{a}^l, \lambda)$ from $\Phi_2$, since $|w|_a < l$ and $|w|_{\overline{a}} < l$.

($\Leftarrow$)
Now suppose that there exists a $\zlclRA$ $M = (\Sigma, \Phi)$ consistent with $S^+$ and $S^-$. We will show that $\psi$ is satisfiable, i.e. we will construct an assignment $v: \Omega \to \{0, 1\}$ such that $v^*(C_i) = 1$ for all $i \in \{1, 2, \ldots, n\}$. First observe, that for each $a \in \Omega$: either $(\lambda, a, \lambda) \in \Phi$, or $(\lambda, \overline{a}, \lambda) \in \Phi$. Consider the positive sample $w_0^+ = (a \overline{a})^l \in S^+$. We know that $(a \overline{a})^l \vdash_M^* \lambda$. Let $\phi \in \Phi$ be the first instruction used in any such accepting computation. The instruction $\phi$ is of the form $(\lambda, z, \lambda)$, where $z$ is a subword of the word $(a \overline{a})^l$. However, the only allowed options here are $\phi \in \{ (\lambda, a, \lambda), (\lambda, \overline{a}, \lambda) \}$, because if $|z| > 1$, then we would immediately get that $|z|_a \ge 1$, $|z|_{\overline{a}} \ge 1$, and thus also $z \in S^-$, which is a contradiction to $z \vdash^{(\phi)} \lambda$. Moreover, both instructions $(\lambda, a, \lambda)$ and $(\lambda, \overline{a}, \lambda)$ cannot be in $\Phi$ simultaneously, because it would mean that $a \overline{a} \vdash_M^* \lambda$, where $a \overline{a} \in S^-$. Now, for each $a \in \Omega$ let $v(a) = 1 \text{ if } (\lambda, a, \lambda) \in \Phi$, and let $v(a) = 0 \text{ if } (\lambda, \overline{a}, \lambda) \in \Phi$. For each clause $C_i = \ell_{i,1} \vee \ell_{i,2} \vee \ell_{i,3}$ we have a negative sample $w_{C_i}^- = \overline{\ell_{i,1}}\ \overline{\ell_{i,2}}\ \overline{\ell_{i,3}} \in S^-$. Therefore, $\overline{\ell_{i,1}} \not\vdash_M \lambda$ or $\overline{\ell_{i,2}} \not\vdash_M \lambda$ or $\overline{\ell_{i,3}} \not\vdash_M \lambda$, which is equivalent to $v(\ell_{i,1}) = 1$ or $v(\ell_{i,2}) = 1$ or $v(\ell_{i,3}) = 1$. This means that $\psi$ is satisfiable.

It remains to be shown that the task of finding a $\zlclRA$ $M = (\Sigma, \Phi)$ consistent with the given input set of positive and negative samples ($S^+$, $S^-$) is in $\NP$. According to Theorem \ref{theorem:clra_membership} the membership problem for $\kclRA[1]$ (and therefore also for $\kclRA[0]$) is decidable in polynomial time. The next question is, how many instructions do we need? It turns out that the number of instructions can be bounded from above by $\size(S^+) = \sum_{w \in S^+} |w|$, because for every positive sample $w_+ \in S^+$ the accepting computation $w_+ \vdash_M^* \lambda$ uses at most $|w_+|$ many instructions. Therefore, we can first nondeterministically guess the set of instructions, and then verify in a polynomial time the consistency with the given input set of positive and negative samples.
\end{proof}

In the following, we prove a more general Theorem \ref{theorem:kclrainference}.

\begin{theorem}\label{theorem:kclrainference}
Let $k \ge 1 $ and $l \ge 4k + 4$ be fixed integers.
It is $\NP$-hard to decide whether there exists a $\klclRA$ $M = (\Sigma, \Phi)$ consistent with a given sample $S = (S^+, S^-)$, $S^+ \cap S^- = \emptyset$, $\lambda \in S^+$.
\end{theorem}

\begin{proof}
Consider a $3$-$\SAT$ formula $\psi = \bigwedge_{i=1}^n C_i$, where clause
$C_i = \ell_{i,1} \vee \ell_{i,2} \vee \ell_{i,3}$, and 
$\ell_{i,1}$, $\ell_{i,2}$, $\ell_{i,3}$
are literals having pairwise different variables, for all $i \in \{1, 2, \ldots, n\}$. 
Let $\Omega = \{a_1, a_2, \ldots, a_m\}$ be the set of all variables occurring in $\psi$.
In the following, we will (effectively) construct a finite sample $S = (S^+, S^-)$, 
$S^+ \cap S^- = \emptyset$, $\lambda \in S^+$, such that the following holds: 
the formula $\psi$ is satisfiable if and only if there exists a $\klclRA$ 
$M = (\Sigma, \Phi)$ consistent with $S = (S^+, S^-)$.
Our alphabet $\Sigma$ will contain all symbols from $\Omega \cup \overline{\Omega}$, where
$\overline{\Omega} = \{ \overline{a_i} \mid a_i \in \Omega \}$, and
$\Omega \cap \overline{\Omega} = \emptyset$. We define $\overline{\overline{a}} = a$
for all $a \in \Omega$. In addition, $\Sigma$ will contain also some 
other special symbols.
First set $S^+ := \{ \lambda \}, S^- := \emptyset$. For each clause 
$C_i = \ell_{i,1} \vee \ell_{i,2} \vee \ell_{i,3}$ add the negative sample
$w_{C_i}^- = \square^k \overline{\ell_{i,1}} \square^k \overline{\ell_{i,2}} 
\square^k \overline{\ell_{i,3}} \square^k$ to the set of negative samples $S^-$,
where $\square$ is a special dummy symbol that will later match the contexts
of the instructions. For each variable $a \in \Omega$, add the following 
positive samples:
$w_0^+ = \square^k a \square^t \overline{a} \square^k$, 
$w_1^+ = \square^k a \square^{k+t}$, 
$w_2^+ = \square^{k+t} \overline{a} \square^k$, and
$w_3^+ = \square^{4k}$
to the set of positive samples $S^+$,
and also add the negative sample 
$w_4^- = \square^k a \square^{2k}  \overline{a} \square^k$ 
to the set of negative samples $S^-$, where $t = l - 2k - 3$. 
Since $l \ge 4k + 4$, it follows that $t \ge 2k + 1$,
and thus $w_0^+ \neq w_4^-$. Our next goal is to allow 
only the following types of instructions, where $a \in \Omega$:
\begin{enumerate}
\item[(a)] $(\square^k, a, \square^k)$.
\item[(b)] $(\square^k, \overline{a}, \square^k)$.
\item[(c)] $(\cent, \square^k a \square^{k+t}, \$)$.
\item[(d)] $(\cent, \square^{k+t} \overline{a} \square^k, \$)$.
\item[(e)] $(\cent, \square^{4k}, \$)$.
\end{enumerate}
All of these instructions have width at most $l$. 
(The longest are the instructions of the type 
(c) and (d), having the width $2k + t + 3 = l$.)
On the other hand, it is not possible to 
store the whole word
$w_0^+ = \square^k a \square^t \overline{a} \square^k$ 
in one instruction containing both sentinels (such as  
$(\cent, \square^k a \square^t \overline{a} \square^k, \$)$),
since its width would be $2k + t + 4 = l + 1 > l$.
Also note, that the instructions (c) and (d) will never 
interfere with any of the negative samples 
$w_{C_i}^- = \square^k \overline{\ell_{i,1}} \square^k \overline{\ell_{i,2}} 
\square^k \overline{\ell_{i,3}} \square^k$,
because: 
$|\square^k \overline{\ell_{i,1}} \square^k \square^k \square^k| = 
|\square^k \square^k \square^k \overline{\ell_{i,3}} \square^k| = 4k + 1$,
while $|\square^k a \square^{k+t}| = |\square^{k+t} \overline{a} \square^k| =
2k + t + 1 = l - 2 \ge 4k + 2$. The same holds for the negative
sample $w_4^- = \square^k a \square^{2k}  \overline{a} \square^k$,
because $|\square^k a \square^{2k} \square^k| = 
|\square^k \square^{2k}  \overline{a} \square^k| = 4k + 1$.

In the following we introduce a general technique, how to prohibit the 
inference of any specific undesirable instruction.
Suppose that we want to prohibit the instruction $\phi = (x, z, y)$,
where $|xzy| \le l$.
Let $x'$ (or $y'$) be the largest possible subword of $x$
(or $y$, respectively) not containing the sentinels ($\cent$, $\$$); thus
either $x = x'$, or $x = \cent x'$ (either $y = y'$, or $y = y'\$$, 
respectively). 
There are only the following four possible cases:
\begin{enumerate}
\item $x = \cent x'$ and $y = y' \$$
\item $x = \cent x'$ and $y = y'$
\item $x = x'$ and $y = y' \$$
\item $x = x'$ and $y = y'$
\end{enumerate}
In the first case we only need to add 
the word $x'zy'$ to the set of negative samples $S^-$ and
the word $x'y'$ to the set of positive samples $S^+$ in order to 
prohibit the instruction $\phi = (x, z, y) = (\cent x', z, y' \$)$.
In the other cases let us first introduce a new symbol $\square_{\phi}$, 
which we also add to our alphabet $\Sigma$.
This is what we do in the particular cases:
\begin{enumerate}
\item $S^- := S^- \cup \{ x'zy' \}$,
$S^+ := S^+ \cup \{  x'y' \}$.
\item $S^- := S^- \cup \{ x'zy'  \square_{\phi} \}$, 
$S^+ := S^+ \cup \{  x'y'  \square_{\phi} \}$.
\item $S^- := S^- \cup \{ \square_{\phi} x'zy' \}$, 
$S^+ := S^+ \cup \{ \square_{\phi} x'y' \}$.
\item $S^- := S^- \cup \{ \square_{\phi} x'zy' \square_{\phi} \}$, 
$S^+ := S^+ \cup \{ \square_{\phi} x'y' \square_{\phi} \}$.
\end{enumerate}
For every prohibited instruction $\phi$ we add two new samples:
a positive sample $w_{\phi}^+$ to the set $S^+$ and a negative
sample $w_{\phi}^-$ to the set $S^-$. 
It is easy to see that in each case we have effectively prohibited
the instruction $\phi = (x, z, y)$. Note that this is the only instruction
not containing the symbol $\square_{\phi}$ (and
having $x \in LC_k, y \in RC_k$) 
that reduces $w_{\phi}^+$ to $w_{\phi}^-$.
Later, we will have to satisfy the consistency of 
the constructed $\klclRA$ $M$ also with these newly added samples, i.\,e.\ that for 
every prohibited instruction $\phi$ the following holds:
$w_{\phi}^+ \vdash_M^* \lambda$ and $w_{\phi}^- \not\vdash_M^* \lambda$.
In all cases, the newly added positive
sample $w_{\phi}^+$ can always be reduced to the empty word in one step
by using the instruction $(\cent, w_{\phi}^+, \$)$. This is because the 
width of this instruction is $2 + |w_{\phi}^+| \le 2 + 2 + |x'y'| \le 4 + 2k \le l$.
If we use a new symbol $\square_{\phi}$ in $w_{\phi}^+$,
then the instruction $(\cent, w_{\phi}^+, \$)$ will be applicable only to this
specific sample $w_{\phi}^+ \in S^+$, and thus will not interfere 
with other samples. On the other hand, the verification of the second 
condition ($w_{\phi}^- \not\vdash_M^* \lambda$) is more difficult. 
Fortunately, we will always use the symbol $\square_{\phi}$ in 
$w_{\phi}^-$, i.\,e.\ the first case ($x = \cent x'$ and $y = y' \$$) 
will never occur in our proof.

Now we have all necessary ingredients to finish the proof.
For every $a \in \Omega$ consider the following 
positive sample:
$w_0^+ = \square^k a \square^t \overline{a} \square^k$.
By using the above technique, we disable all instructions
applicable to this word having the width at most $l$ 
except for the instructions of the form (a) -- (e). 
Observe that we will never attempt to disable
any instruction of the form $\phi = (\cent x', z, y' \$)$.
This is because the word 
$w_0^+ = \square^k a \square^t \overline{a} \square^k$
(as we have already mentioned above) is too long.
Moreover, there is only polynomially many
disabled instructions, since there is only polynomially
many subwords of the above word.
Now we have completely specified the sets of positive 
and negative samples $S^+$, $S^-$, $S^+ \cap S^- = \emptyset$,
$\lambda \in S^+$, and thus we can proceed with the proof.

($\Rightarrow$)
Suppose that $\psi$ is satisfiable, i.\,e.\ there exists an assignment
$v: \Omega \to \{0, 1\}$ such that $v^*(C_i) = 1$ for all $i \in \{1, 2, \ldots, n\}$,
where $v^*$ denotes the natural extension of $v$ to formulas.
We will show that there exists a $\klclRA$ $M = (\Sigma, \Phi)$ 
consistent with $S = (S^+, S^-)$.
Consider the following $\klclRA$ $M = (\Sigma, \Phi)$: First,  
add to $\Phi$ the following set of instructions: 
$\Phi_1 = \{ (\square^k, a, \square^k) \mid a \in \Omega: v(a) = 1 \} \cup
\{ (\square^k, \overline{a}, \square^k) \mid a \in \Omega: v(a) = 0 \}$.
It can be easily observed that, for all literals $l \in \Omega \cup \overline{\Omega}:
\square^k l \square^k \vdash_M \square^k \square^k \Leftrightarrow v(l) = 1$, 
or equivalently:
$\square^k \overline{l} \square^k \vdash_M \square^k \square^k \Leftrightarrow v(l) = 0$. 
Therefore no negative sample 
$w_{C_i}^- = \square^k \overline{\ell_{i,1}} \square^k \overline{\ell_{i,2}} 
\square^k \overline{\ell_{i,3}} \square^k$,
where $i \in \{1, 2, \ldots, n\}$, can be reduced to the positive sample 
$\square^{4k}$ by using the instructions from $\Phi_1$, because otherwise 
it would mean that $v(\ell_{i,1}) = v(\ell_{i,2}) = v(\ell_{i,3}) = 0$.
Next, add to $\Phi$ the following set of instructions 
$\Phi_2 = \{ (\cent, \square^k a \square^{k+t}, \$), 
(\cent, \square^{k+t} \overline{a} \square^k, \$)
\mid a \in \Omega \} \cup \{ (\cent, \square^{4k}, \$) \}$.
As we have already stated above, no instruction from $\Phi_2$ will ever interfere
with any negative sample 
$w_{C_i}^- = \square^k \overline{\ell_{i,1}} \square^k \overline{\ell_{i,2}} 
\square^k \overline{\ell_{i,3}} \square^k$, or 
$w_4^- = \square^k a \square^{2k}  \overline{a} \square^k$.
Finally, add to $\Phi$ all instructions $(\cent,  w_{\phi}^+, \$)$, 
where $ w_{\phi}^+$ was added to the set of positive samples $S^+$ 
during the process of disabling the undesirable instruction $\phi$.
These instructions are applicable only to words containing the special
symbols $\square_{\phi}$, so we do not have to care about them at all.
Now we will show that the constructed $\klclRA$ $M = (\Sigma, \Phi)$ 
is consistent with the sample $S = (S^+, S^-)$.
(The width of instructions of $M$ is obviously bounded from above by $l$.)
First, it is easy to see, that for each variable $a \in \Omega$ the following
positive samples:
$w_0^+ = \square^k a \square^t \overline{a} \square^k$, 
$w_1^+ = \square^k a \square^{k+t}$, 
$w_2^+ = \square^{k+t} \overline{a} \square^k$,
$w_3^+ = \square^{4k}$
are all reducible to the empty word $\lambda$.
The positive sample $w_0^+$ is always reducible to either $w_1^+$,
or $w_2^+$, depending on whether $(\square^k, a, \square^k) \in \Phi_1$,
or $(\square^k, \overline{a}, \square^k) \in \Phi_1$.
The other positive samples: $w_1^+$, $w_2^+$, $w_3^+$ can be reduced
to the empty word $\lambda$ in one single step by using the corresponding 
instruction from $\Phi_2$. The negative sample 
$w_4^- = \square^k a \square^{2k} \overline{a} \square^k$ clearly
cannot be reduced to the empty word $\lambda$. We can clear either 
the letter $a$, or $\overline{a}$ by using the corresponding 
instruction from $\Phi_1$, but then we get the irreducible 
word $\square^k \square^{2k} \overline{a} \square^k$, or 
$\square^k a \square^{2k} \square^k$. None of the instructions from $\Phi_2$ 
can be applied to such a word, since the length 
$|\square^k a \square^{2k} \square^k| = 
|\square^k \square^{2k} \overline{a} \square^k| = 4k + 1$,
while $|\square^k a \square^{k+t}| = |\square^{k+t} \overline{a} \square^k| =
2k + t + 1 = l - 2 \ge 4k + 2$.
It remains to be shown, that no negative sample $w_{\phi}^-$, which was added 
to the set of negative samples $S^-$ during the process of disabling 
some instruction $\phi$, can be reduced to the empty word $\lambda$. 
Both the negative sample $w_{\phi}^-$ and the corresponding positive 
sample $w_{\phi}^+$ contain the special symbol $\square_{\phi}$. 
The negative sample $w_{\phi}^-$ without this special symbol 
$\square_{\phi}$ is basically a subword of some word 
$\square^k a \square^t \overline{a} \square^k$.
First observe that the only instruction from $\Phi$ that could be possibly applied 
to the negative sample $w_{\phi}^-$, is some instruction from $\Phi_1$. Without 
loss of generality assume that we can apply the instruction
$\rho = (\square^k, a, \square^k) \in \Phi_1$ to the word $w_{\phi}^-$, i.\,e.\
$w_{\phi}^- \vdash^{(\rho)} w'$. 
This also implies that $(\square^k, \overline{a}, \square^k) \notin \Phi_1$.
It is easy to see that no other instruction 
from $\Phi$ can be applied to the resulting word $w'$, except possibly 
the instruction $(\cent, w_{\phi}^+, \$)$. But if $(\cent, w_{\phi}^+, \$)$ was
applicable to $w'$, it would have implied that we wanted to disable the
instruction $\rho$ itself, which is not possible, since $\rho$ is of 
the form (a). Thus, we have shown that the constructed $\klclRA$ $M = (\Sigma, \Phi)$ 
is consistent with $S = (S^+, S^-)$. The size of the constructed set of positive 
and negative samples is linear with respect to the size of the formula $\psi$.

($\Leftarrow$)
Now suppose that there exists a $\klclRA$ $M = (\Sigma, \Phi)$ consistent
with $S = (S^+, S^-)$. We will show that 
$\psi$ is satisfiable, i.\,e.\ we will construct an assignment
$v: \Omega \to \{0, 1\}$ such that $v^*(C_i) = 1$ for all $i \in \{1, 2, \ldots, n\}$.
First observe that for each $a \in \Omega$: either $(\square^k, a, \square^k) \in \Phi$,
or $(\square^k, \overline{a}, \square^k) \in \Phi$. Consider the positive sample
$w_0^+ = \square^k a \square^t \overline{a} \square^k$. 
We know that $\square^k a \square^t \overline{a} \square^k \vdash_M^* \lambda$.
Let $\phi \in \Phi$ be the first instruction used in any such accepting computation.
The instruction $\phi$ is either of the form $(\square^k, a, \square^k)$,
or $(\square^k, \overline{a}, \square^k)$, because all other instructions
are disabled. Moreover, it cannot happen that both instructions
$(\square^k, a, \square^k)$, $(\square^k, \overline{a}, \square^k)$ are in $\Phi$, because
it would mean that 
$\square^k a \square^{2k} \overline{a} \square^k \vdash_M^* \square^{4k}$, 
where $\square^k a \square^{2k} \overline{a} \square^k \in S^-$ and 
$\square^{4k} \in S^+$.
Now let us define the assignment $v: \Omega \to \{0, 1\}$ as follows:
for each $a \in \Omega:$ $v(a) = 1 \text{ if } (\lambda, a, \lambda) \in \Phi$,
and $v(a) = 0 \text{ if } (\lambda, \overline{a}, \lambda) \in \Phi$.
For each clause $C_i = \ell_{i,1} \vee \ell_{i,2} \vee \ell_{i,3}$ we have
a negative sample 
$w_{C_i}^- = \square^k \overline{\ell_{i,1}} \square^k \overline{\ell_{i,2}} 
\square^k \overline{\ell_{i,3}} \square^k \in S^-$.
Therefore, 
$(\square^k, \overline{\ell_{i,1}}, \square^k) \notin \Phi$ or
$(\square^k, \overline{\ell_{i,2}}, \square^k) \notin \Phi$ or
$(\square^k, \overline{\ell_{i,3}}, \square^k) \notin \Phi$,
which is equivalent to $v(\ell_{i,1}) = 1$ or $v(\ell_{i,2}) = 1$ or $v(\ell_{i,3}) = 1$.
This means that $\psi$ is satisfiable.
\end{proof}

\subsection{Learning Without Restrictions}\label{section:unrestricted-learning}

Now we turn to learning without the restricted width of instructions. As we have already stated we will use the auxiliary inference procedure (Algorithm \ref{algorithm:lambda-learning}) from Section \ref{section:restricted-learning} as a component in our learning algorithm. The inference procedure itself requires the specification of the maximal width of instructions $l \ge 1$, so it is only natural to try all widths $l = 1, 2, \ldots$, until the inference procedure returns a consistent model with the given input sample. The last missing bit is making the learning algorithm a polynomial time algorithm w.r.t. the size of the input sample. In order to do so, we restrict ourselves to the left-most rewriting. The resulting learning algorithm is shown in Algorithm \ref{algorithm:unconstrained-lambda-learning}. Algorithm \ref{algorithm:consistency} shows the implementation of the consistency check function $\mathsf{Consistent}$ and Algorithm \ref{algorithm:simplification} shows the implementation of the simplification function $\mathsf{Simplify}$. Note that in Step \ref{algorithm:unconstrained-lambda-learning:infer} when calling the auxiliary inference procedure $\mathsf{Infer}_{\calM}(S, k, l)$ we do not use the left-most rewriting.

\begin{algorithm}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\caption{Learning algorithm $\mathsf{UnconstrainedInfer}_{\calM}(S, k)$}
\label{algorithm:unconstrained-lambda-learning}
%\DontPrintSemicolon
\LinesNumbered
\Input{Sample $S = (S^+, S^-)$ over $\Sigma$,
$S^+ \cap S^- = \emptyset, \lambda \in S^+$.\\
Length of contexts $k \ge 1$, or $k = \cdot$, if not specified.}
\Output{A $\kCRS$ $M \in \leftcalM$ consistent with $S$.}
$l_{\max} \leftarrow 2 + \max\{ |w| \mid w \in S^+ \}$\;
\For{$l = 1 \ldots l_{\max}$\label
{algorithm:unconstrained-lambda-learning:cycle-start}}
{$M \leftarrow \mathsf{Infer}_{\calM}(S, k, l)$\label{algorithm:unconstrained-lambda-learning:infer}\;
\If{$\mathsf{Consistent}(M_{\sf left}, S)$\label{algorithm:unconstrained-lambda-learning:consistent}}{\Return{$\mathsf{Simplify}(M_{\sf left})$\label{algorithm:unconstrained-lambda-learning:simplification}}\;\label
{algorithm:unconstrained-lambda-learning:cycle-end}}}
\Return{$M_{\sf left}$ with instructions $\Phi = \{(\cent, w \to \lambda, \$) \mid w \in S^+\}$}\label{algorithm:unconstrained-lambda-learning:end}\;
\end{algorithm}

In turns out that Algorithm \ref{algorithm:unconstrained-lambda-learning} is not only able to identify any target $\lambda$-confluent model in the limit in polynomial time, it also infers a simplified $\lambda$-confluent model with \emph{minimal width} in the limit.

\begin{theorem}\label{theorem:unconstrained-lambda-inference}
Let $\calM$ be a class of $\kCRS$ restricted according to Definition \ref{definition:restrictions} (where $k \ge 1$ or $k = \cdot$ if not used) and $\calLL$ be the class of languages $\calL{\lconcalM}$. Let function $\mathsf{Assumptions}$ (used in the auxiliary inference procedure $\mathsf{Infer}_{\calM}$) be monotone and correct with respect to $\calM$. Then:
\begin{enumerate}
\item\label{unconstrained-lambda-inference1} Given a sample $S = (S^+, S^-)$ for $L \in \calLL$, $\mathsf{UnconstrainedInfer}_{\calM}(S, k)$ returns a model $N \in \leftcalM$ \emph{consistent with} $S$ in polynomial time w.r.t. $\size(S)$.

\item\label{unconstrained-lambda-inference2} For every model $M \in \lconcalM$ representing the language $L \in \calLL$, there exists a finite \emph{characteristic sample} $S_0 = (S_0^+, S_0^-)$ and a simplified $\lambda$-confluent model $N \in \leftcalM$ with \emph{minimal width} equivalent to $M$ such that, on all samples $S = (S^+, S^-)$ for $L$ that satisfy $S_0^+ \subseteq S^+$ and $S_0^- \subseteq S^-$, $\mathsf{UnconstrainedInfer}_{\calM}(S, k)$ returns $N$.
\end{enumerate}
\end{theorem}

\begin{proof} (\ref{unconstrained-lambda-inference1}) Loop \ref{algorithm:unconstrained-lambda-learning:cycle-start} -- \ref{algorithm:unconstrained-lambda-learning:cycle-end}  consists of at most $2 + \size(S)$ iterations. Step \ref{algorithm:unconstrained-lambda-learning:infer} runs in polynomial time w.r.t. $\size(S)$ and since in Steps \ref{algorithm:unconstrained-lambda-learning:consistent} and \ref{algorithm:unconstrained-lambda-learning:simplification} we use only left-most reductions, both consistency check and simplification run in polynomial time w.r.t. $\size(S)$. Therefore, Algorithm \ref{algorithm:unconstrained-lambda-learning} runs in polynomial time w.r.t. $\size(S)$. Finally, in both Steps \ref{algorithm:unconstrained-lambda-learning:cycle-end} and \ref{algorithm:unconstrained-lambda-learning:end} we return a simplified model from $\leftcalM$ consistent with $S$. 

\noindent(\ref{unconstrained-lambda-inference2}) Without loss of generality we may assume that the model $M = (\Sigma, \Phi) \in \lconcalM$ representing the target language $L$ has minimal width $|M| = l_0$ and a minimal set of instructions. Similarly as in the proof of Theorem \ref{theorem:lambda-inference} we can define the term \emph{bad} instruction with respect to the target language $L(M)$. Let $\Theta \supseteq \Phi$ denote the set of all legal instruction of the class $\kllcalM$ that are not bad w.r.t. $L(M)$. Then apparently $\bar{M} := (\Sigma, \Theta) \in \kllcalM$, $\bar{M}$ is $\lambda$-confluent and $L(\bar{M}) = L(M)$. In addition, there exists a word $w_0 \in L(\bar{M})$ such that every accepting computation $w_0 \vdash_{\bar{M}}^* \lambda$ must use some instruction $\phi \in \Theta$ with $|\phi| = l_0$. (Otherwise the width $|M| = l_0$ would not be minimal.) According to Theorem \ref{theorem:lambda-inference} there exists a characteristic sample $S_0 = (S_0^+, S_0^-)$ and a $\lambda$-confluent model $N \in \kllcalM$ equivalent to $M$ such that, on all samples $S = (S^+, S^-)$ for $L(M)$ that satisfy $S_0^+ \subseteq S^+$ and $S_0^- \subseteq S^-$, the inference procedure $\mathsf{Infer}_{\calM}(S, k, l_0)$ returns $N$. We may assume that $w_0 \in S_0^+$. According to Lemma \ref{lemma:lambda} $L(N) = L(N_{\sf left})$, therefore the function $\mathsf{Consistent}$ in Step \ref{algorithm:unconstrained-lambda-learning:consistent} returns ${\bf True}$ and in Step \ref{algorithm:unconstrained-lambda-learning:simplification} we return $\mathsf{Simplify}(N_{\sf left})$, which is a simplified $\lambda$-confluent model with minimal width $l_0$ equivalent to $M$ obtained from $N_{\sf left}$. Note that the function $\mathsf{Simplify}$ does not break the property of $\lambda$-confluence. This follows from the fact that the function $\mathsf{Simplify}$ preserves the original rewriting relation $\vdash_N^*$ and therefore also $\lambda$-confluence. Nevertheless, in order to complete the proof, we need to show that Loop \ref{algorithm:unconstrained-lambda-learning:cycle-start} -- \ref{algorithm:unconstrained-lambda-learning:cycle-end} will get to $l = l_0$. In other words, we need to show that for every $l < l_0$ the inference procedure $\mathsf{Infer}_{\calM}(S, k, l)$ will return a model $N \in \klcalM$ such that the function $\mathsf{Consistent}$ in Step \ref{algorithm:unconstrained-lambda-learning:consistent} will return ${\bf False}$. For every $l < l_0: \mathsf{Assumptions}(S^+, k, l) \subset \mathsf{Assumptions}(S^+, k, l_0)$. This is because the set $\mathsf{Assumptions}(S^+, k, l_0)$ contains all instructions of $M$ (this follows from the minimality of $\Phi$), and some of these instructions have the width equal to $l_0$. Since all bad instructions from $\mathsf{Assumptions}(S^+, k, l_0)$ are disabled by $(S^+, S^-)$, the inference procedure $\mathsf{Infer}_{\calM}(S, k, l)$ will filter all bad instructions returned by $\mathsf{Assumptions}(S^+, k, l)$. The resulting set of instructions $\Phi'$ will contain only the correct instructions, i.\,e., $\Phi' \subseteq \Theta$. However, the instructions of $M$ that have the width equal to $l_0$ will be missing in this set $\Phi'$. This implies that the word $w_0 \in S_0^+$ cannot be reduced to $\lambda$ by using only the instructions from $\Phi'$. This also implies that the word $w_0$ cannot be reduced to $\lambda$ by using left-most reductions w.r.t. $\Phi'$. Therefore, the function $\mathsf{Consistent}$ in Step \ref{algorithm:unconstrained-lambda-learning:consistent} will return ${\bf False}$ and we will proceed to the next iteration.
\end{proof}

\begin{algorithm}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\caption{Implementation of $\mathsf{Consistent}(M, S)$}
\label{algorithm:consistency}
%\DontPrintSemicolon
\LinesNumbered
\Input{A $\CRS$ $M = (\Sigma, \Phi)$.\\
Sample $S = (S^+, S^-)$ over $\Sigma$, 
$S^+ \cap S^- = \emptyset, \lambda \in S^+$.}
\Output{${\bf True}$ if $M$ consistent with $S$. Otherwise ${\bf False}$.}
\If{$\forall w \in S^+: w \vdash_M^* \lambda \wedge 
\forall w \in S^-: w \not\vdash_M^* \lambda$}
{\Return{${\bf True}$}\;}
\Return{${\bf False}$}\;
\end{algorithm}

\begin{algorithm}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\caption{Implementation of $\mathsf{Simplify}(M)$}
\label{algorithm:simplification}
%\DontPrintSemicolon
\LinesNumbered
\Input{A $\CRS$ $M = (\Sigma, \Phi)$.}
\Output{A simplified $\CRS$ $M = (\Sigma, \Phi)$.}
\While{$\exists \phi = (x, z \to t, y) \in \Phi:
z \vdash_{\Phi - \{\phi\}}^* t$ in the context $(x, y)$}
{$\Phi = \Phi - \{\phi\}$}
\Return{$(\Sigma, \Phi)$}\;
\end{algorithm}

Note that the order of the removed instructions from $\Phi$ in the function $\mathsf{Simplify}$ (Algorithm \ref{algorithm:simplification}) does not matter as long as we are consistent. According to Definition \ref{definition:crs} we always get a simplified $\CRS$, although there may exist several equivalent simplified $\CRS$. The reason why we have included the simplification function $\mathsf{Simplify}$ in our Algorithm \ref{algorithm:unconstrained-lambda-learning} is twofold. First, our experiments have shown that the simplification function can substantially reduce the resulting set of instructions. Second, imagine a long sequence of reductions: $w_0 \vdash_M^{(\phi_1)} w_1 \ldots \vdash_M^{(\phi_n)} w_n = \lambda$. For each $0 \le i < j - 1 < n$ there is a chance that the learning algorithm will infer also a \emph{shortcut} instruction $\phi_{ij}$ such that $w_i \vdash_M^{(\phi_{ij})} w_j$. Such shortcut instructions are useless and there may be $O(n^2)$ shortcut instructions for every $n$-step reduction sequence. All shortcut instructions will be removed when using the simplification function $\mathsf{Simplify}$.
