\chapter*{Conclusion}\label{chapter:conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

We have successfully achieved all goals of this thesis and the presented results were published in 5 proceedings and 2 journal papers. We have studied locally restricted models of restarting automata, starting with the most restricted model, the so-called clearing restarting automaton, which, based on a limited context, can only delete a substring of the current content of its tape, and then gradually extending its power either by relaxing the constraints put on its instructions, or by allowing auxiliary symbols. In the first part of the thesis we have focused solely on models without auxiliary symbols and compared them to Chomsky hierarchy. Additionally, we have developed a general learning framework, under which it is possible to effectively infer various types of $\lambda$-confluent models from informant (i.\,e.\ from positive and negative samples) under a slightly relaxed polynomial learning paradigm. In the second part of the thesis we have investigated models with auxiliary symbols, starting with the so-called $\Delta$-clearing restarting automata and then moving on to limited context restarting automata. $\Delta$-clearing restarting automata are very limited in their operations. They can, in one step, either completely delete a subword or they can delete a subword and simultaneously mark its position by a symbol $\Delta$. Surprisingly, they can accept all context-free languages. For this purpose, we have designed a rather complicated coding of information used for encoding nonterminals during a bottom-up analysis. Limited context restarting automata, on the other hand, can use any number of auxiliary symbols. We have compared them with certain McNaughton families of languages and proved that the class of growing context-sensitive languages is an upper bound for all the types of limited context restarting automata considered, and that this upper bound is attained by three classes of these automata. Under the additional requirement of confluence, the Church-Rosser languages form an upper bound, which is reached by the three most general types of these automata. On the other hand, for the most restricted types of limited context restarting automata, we just obtain the regular languages, both in the confluent and the non-confluent case.

We have provided a solid ground for further research, as there are still many interesting open problems and various uncovered and potentially fruitful research directions. In the following we mention some of these problems:

\begin{itemize}
\item It is open whether $\kclRA[2]$ are able to recognize a non-context free language over a two-letter alphabet.
\item We have shown that the membership problem for $\clRA$ is $\classNP$-complete. However, it is still open whether the membership problem is $\classNP$-complete also for $\kclRA$ (or $\kCRS$, in general) with fixed (or bounded) length of contexts $k$. In fact, this is the only obstacle for providing an efficient learning algorithm for inferring the ordinary (non-confluent) $\kclRA$ ($\kCRS$, respectively), since the presented learning algorithm can be easily modified for this purpose.
\item Is there an efficient learning algorithm for confluent $\CRS$ (with or without auxiliary symbols)?
\item Are $\DXclRA$ strictly more powerful than $\DclRA$?
\item Is the class $\GCSL$ a strict upper bound for $\calL{\DXclRA}$ (or $\calL{\DclRA}$)?
\end{itemize}
%
The following research directions might be worth investigating:
%
\begin{itemize}
\item $\sclRA$ and other similar $\CRS$ without auxiliary symbols are still not properly investigated.
\item It could be interesting to consider models with state, or use $\CRS$ in $\mathsf{CD}$-systems.
\item It could be interesting to implement the presented learning algorithms and perform practical experiments on some benchmark inference problems.
\end{itemize}